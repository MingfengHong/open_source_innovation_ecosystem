"""
è§’è‰²è½¬æ¢è·¯å¾„åˆ†ææ¨¡å—
åˆ†æç”¨æˆ·åœ¨å¼€æºç”Ÿæ€ç³»ç»Ÿä¸­çš„è§’è‰²æ¼”åŒ–å’Œè½¬æ¢è·¯å¾„
"""

import pandas as pd
import numpy as np
import networkx as nx
from typing import Dict, List, Tuple, Optional, Any
import logging
from collections import defaultdict, Counter
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

from ..utils.logging_config import setup_logger
from config.settings import ANALYSIS_OUTPUT_DIR, VISUALIZATION_CONFIG

# è®¾ç½®æ—¥å¿—
logger = setup_logger(__name__)


class RoleTransitionAnalyzer:
    """
    è§’è‰²è½¬æ¢è·¯å¾„åˆ†æå™¨
    åˆ†æç”¨æˆ·è§’è‰²çš„æ¼”åŒ–æ¨¡å¼å’Œè½¬æ¢æ¦‚ç‡
    """
    
    def __init__(self, 
                 user_roles_df: pd.DataFrame,
                 activity_data: pd.DataFrame = None,
                 time_window_months: int = 6):
        """
        åˆå§‹åŒ–è§’è‰²è½¬æ¢åˆ†æå™¨
        
        Args:
            user_roles_df: ç”¨æˆ·è§’è‰²æ•°æ®æ¡†
            activity_data: ç”¨æˆ·æ´»åŠ¨æ•°æ®ï¼ˆå¯é€‰ï¼Œç”¨äºé¢„æµ‹è½¬æ¢ï¼‰
            time_window_months: åˆ†ææ—¶é—´çª—å£
        """
        self.user_roles_df = user_roles_df.copy()
        self.activity_data = activity_data
        self.time_window_months = time_window_months
        
        # è§’è‰²æ˜ å°„ï¼ˆä¸å…±ç”Ÿåˆ†æä¿æŒä¸€è‡´ï¼‰
        self.role_mapping = self._create_role_mapping()
        
        # å­˜å‚¨åˆ†æç»“æœ
        self.transition_matrix = None
        self.transition_probabilities = None
        self.pathway_analysis = {}
        self.prediction_model = None
        
        logger.info(f"åˆå§‹åŒ–è§’è‰²è½¬æ¢åˆ†æå™¨: {len(user_roles_df)} ä¸ªç”¨æˆ·, {len(self.role_mapping)} ç§è§’è‰²")
    
    def _create_role_mapping(self) -> Dict[int, str]:
        """åˆ›å»ºè§’è‰²æ˜ å°„"""
        role_mapping = {}
        
        if 'cluster' not in self.user_roles_df.columns:
            logger.warning("ç”¨æˆ·æ•°æ®ä¸­ç¼ºå°‘clusteråˆ—ï¼Œä½¿ç”¨é»˜è®¤è§’è‰²åˆ†é…")
            return {i: f"role_{i}" for i in range(6)}
        
        # åŸºäºèšç±»ç‰¹å¾åˆ†æç¡®å®šè§’è‰²åç§°
        cluster_stats = self.user_roles_df.groupby('cluster').agg({
            'pr_count': 'mean',
            'issue_count': 'mean', 
            'star_count': 'mean',
            'repo_count': 'mean',
            'code_focus_ratio': 'mean',
            'interaction_diversity': 'mean'
        }).fillna(0)
        
        for cluster_id, stats in cluster_stats.iterrows():
            if stats['code_focus_ratio'] > 0.7 and stats['pr_count'] > stats.mean()['pr_count']:
                role_name = "core_developer"
            elif stats['interaction_diversity'] > 3 and stats['issue_count'] > stats.mean()['issue_count']:
                role_name = "community_facilitator"
            elif stats['repo_count'] > stats.mean()['repo_count'] and stats['code_focus_ratio'] > 0.5:
                role_name = "architect"
            elif stats['issue_count'] > stats['pr_count'] and stats['interaction_diversity'] > 2:
                role_name = "problem_solver"
            elif stats['star_count'] > stats.mean()['star_count'] and stats['pr_count'] < stats.mean()['pr_count']:
                role_name = "observer"
            else:
                role_name = "casual_contributor"
            
            role_mapping[cluster_id] = role_name
        
        return role_mapping
    
    def analyze_role_transitions(self, 
                                temporal_user_data: pd.DataFrame = None) -> Dict[str, Any]:
        """
        åˆ†æè§’è‰²è½¬æ¢æ¨¡å¼
        
        Args:
            temporal_user_data: æ—¶é—´åºåˆ—ç”¨æˆ·æ•°æ®ï¼ŒåŒ…å«user_id, time_period, clusterç­‰
            
        Returns:
            Dict[str, Any]: è½¬æ¢åˆ†æç»“æœ
        """
        logger.info("å¼€å§‹åˆ†æè§’è‰²è½¬æ¢æ¨¡å¼...")
        
        if temporal_user_data is None:
            # å¦‚æœæ²¡æœ‰æ—¶é—´åºåˆ—æ•°æ®ï¼Œåˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ç”¨äºæ¼”ç¤º
            temporal_user_data = self._create_simulated_temporal_data()
        
        # 1. è®¡ç®—è½¬æ¢çŸ©é˜µ
        transition_matrix = self._calculate_transition_matrix(temporal_user_data)
        
        # 2. åˆ†æè½¬æ¢è·¯å¾„
        pathway_analysis = self._analyze_transition_pathways(temporal_user_data)
        
        # 3. è®¡ç®—è½¬æ¢æ¦‚ç‡å’Œç¨³å®šæ€§
        stability_analysis = self._analyze_role_stability(temporal_user_data)
        
        # 4. è¯†åˆ«å…³é”®è½¬æ¢è§¦å‘å› ç´ 
        trigger_factors = self._identify_transition_triggers(temporal_user_data)
        
        # 5. é¢„æµ‹æœªæ¥è½¬æ¢
        prediction_results = self._predict_role_transitions(temporal_user_data)
        
        results = {
            'transition_matrix': transition_matrix,
            'pathway_analysis': pathway_analysis,
            'stability_analysis': stability_analysis,
            'trigger_factors': trigger_factors,
            'prediction_results': prediction_results,
            'summary_statistics': self._calculate_transition_summary(transition_matrix, stability_analysis)
        }
        
        # å­˜å‚¨ç»“æœ
        self.transition_matrix = transition_matrix
        self.pathway_analysis = pathway_analysis
        
        # ä¿å­˜ç»“æœ
        self._save_transition_results(results)
        
        return results
    
    def _create_simulated_temporal_data(self) -> pd.DataFrame:
        """åˆ›å»ºæ¨¡æ‹Ÿçš„æ—¶é—´åºåˆ—ç”¨æˆ·æ•°æ®"""
        logger.info("åˆ›å»ºæ¨¡æ‹Ÿæ—¶é—´åºåˆ—æ•°æ®...")
        
        temporal_data = []
        time_periods = pd.date_range('2023-01-01', '2023-12-31', freq='M')
        
        for user_id in self.user_roles_df['user_id'].unique()[:100]:  # é™åˆ¶ç”¨æˆ·æ•°é‡
            current_role = self.user_roles_df[self.user_roles_df['user_id'] == user_id]['cluster'].iloc[0]
            
            for period in time_periods:
                # æ¨¡æ‹Ÿè§’è‰²è½¬æ¢æ¦‚ç‡
                transition_prob = np.random.random()
                
                if transition_prob < 0.1:  # 10%çš„æ¦‚ç‡å‘ç”Ÿè½¬æ¢
                    # éšæœºé€‰æ‹©æ–°è§’è‰²ï¼ˆå€¾å‘äºç›¸é‚»è§’è‰²ï¼‰
                    possible_roles = list(self.role_mapping.keys())
                    # ç§»é™¤å½“å‰è§’è‰²
                    if current_role in possible_roles:
                        possible_roles.remove(current_role)
                    
                    if possible_roles:
                        new_role = np.random.choice(possible_roles)
                        current_role = new_role
                
                temporal_data.append({
                    'user_id': user_id,
                    'time_period': period,
                    'cluster': current_role,
                    'role': self.role_mapping[current_role]
                })
        
        return pd.DataFrame(temporal_data)
    
    def _calculate_transition_matrix(self, temporal_data: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—è§’è‰²è½¬æ¢çŸ©é˜µ"""
        logger.info("è®¡ç®—è§’è‰²è½¬æ¢çŸ©é˜µ...")
        
        roles = list(self.role_mapping.values())
        transition_counts = pd.DataFrame(0, index=roles, columns=roles)
        
        # æŒ‰ç”¨æˆ·åˆ†ç»„ï¼Œåˆ†æè§’è‰²è½¬æ¢
        for user_id, user_data in temporal_data.groupby('user_id'):
            user_data = user_data.sort_values('time_period')
            
            # è®¡ç®—è¿ç»­æ—¶æœŸçš„è§’è‰²è½¬æ¢
            for i in range(len(user_data) - 1):
                current_role = user_data.iloc[i]['role']
                next_role = user_data.iloc[i + 1]['role']
                
                if current_role in roles and next_role in roles:
                    transition_counts.loc[current_role, next_role] += 1
        
        # è½¬æ¢ä¸ºæ¦‚ç‡çŸ©é˜µ
        transition_matrix = transition_counts.div(transition_counts.sum(axis=1), axis=0).fillna(0)
        
        return transition_matrix
    
    def _analyze_transition_pathways(self, temporal_data: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æè½¬æ¢è·¯å¾„"""
        logger.info("åˆ†æè§’è‰²è½¬æ¢è·¯å¾„...")
        
        pathway_analysis = {}
        
        # 1. è¯†åˆ«æœ€å¸¸è§çš„è½¬æ¢è·¯å¾„
        common_transitions = self._find_common_transition_sequences(temporal_data)
        pathway_analysis['common_sequences'] = common_transitions
        
        # 2. åˆ†ææ–°æ‰‹åˆ°ä¸“å®¶çš„è·¯å¾„
        novice_to_expert_paths = self._analyze_novice_expert_pathways(temporal_data)
        pathway_analysis['novice_expert_paths'] = novice_to_expert_paths
        
        # 3. è®¡ç®—è·¯å¾„é•¿åº¦åˆ†å¸ƒ
        path_length_distribution = self._calculate_path_length_distribution(temporal_data)
        pathway_analysis['path_length_distribution'] = path_length_distribution
        
        # 4. è¯†åˆ«å…³é”®ä¸­ä»‹è§’è‰²
        bridge_roles = self._identify_bridge_roles(temporal_data)
        pathway_analysis['bridge_roles'] = bridge_roles
        
        return pathway_analysis
    
    def _find_common_transition_sequences(self, temporal_data: pd.DataFrame, max_length: int = 4) -> List[Dict]:
        """æ‰¾åˆ°æœ€å¸¸è§çš„è½¬æ¢åºåˆ—"""
        sequences = defaultdict(int)
        
        # æå–æ¯ä¸ªç”¨æˆ·çš„è§’è‰²åºåˆ—
        for user_id, user_data in temporal_data.groupby('user_id'):
            user_data = user_data.sort_values('time_period')
            role_sequence = user_data['role'].tolist()
            
            # æå–æ‰€æœ‰å¯èƒ½çš„å­åºåˆ—
            for length in range(2, min(max_length + 1, len(role_sequence) + 1)):
                for i in range(len(role_sequence) - length + 1):
                    subseq = tuple(role_sequence[i:i + length])
                    sequences[subseq] += 1
        
        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åº
        common_sequences = [
            {'sequence': list(seq), 'count': count, 'length': len(seq)}
            for seq, count in sequences.items()
            if count >= 2  # è‡³å°‘å‡ºç°2æ¬¡
        ]
        
        common_sequences.sort(key=lambda x: x['count'], reverse=True)
        
        return common_sequences[:20]  # è¿”å›å‰20ä¸ªæœ€å¸¸è§çš„åºåˆ—
    
    def _analyze_novice_expert_pathways(self, temporal_data: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†ææ–°æ‰‹åˆ°ä¸“å®¶çš„è·¯å¾„"""
        # å®šä¹‰æ–°æ‰‹å’Œä¸“å®¶è§’è‰²
        novice_roles = ['observer', 'casual_contributor']
        expert_roles = ['core_developer', 'architect', 'community_facilitator']
        
        pathways = []
        
        for user_id, user_data in temporal_data.groupby('user_id'):
            user_data = user_data.sort_values('time_period')
            roles = user_data['role'].tolist()
            
            # å¯»æ‰¾ä»æ–°æ‰‹è§’è‰²å¼€å§‹ï¼Œåˆ°ä¸“å®¶è§’è‰²ç»“æŸçš„è·¯å¾„
            start_indices = [i for i, role in enumerate(roles) if role in novice_roles]
            end_indices = [i for i, role in enumerate(roles) if role in expert_roles]
            
            for start_idx in start_indices:
                for end_idx in end_indices:
                    if end_idx > start_idx:
                        pathway = roles[start_idx:end_idx + 1]
                        pathways.append({
                            'user_id': user_id,
                            'pathway': pathway,
                            'length': len(pathway),
                            'duration': end_idx - start_idx
                        })
                        break  # åªå–ç¬¬ä¸€ä¸ªä¸“å®¶è§’è‰²
        
        # åˆ†æè·¯å¾„ç»Ÿè®¡
        if pathways:
            avg_length = np.mean([p['length'] for p in pathways])
            avg_duration = np.mean([p['duration'] for p in pathways])
            
            # æœ€å¸¸è§çš„è·¯å¾„
            pathway_counts = Counter(tuple(p['pathway']) for p in pathways)
            most_common_paths = [
                {'pathway': list(path), 'count': count}
                for path, count in pathway_counts.most_common(10)
            ]
        else:
            avg_length = 0
            avg_duration = 0
            most_common_paths = []
        
        return {
            'total_pathways_found': len(pathways),
            'average_pathway_length': avg_length,
            'average_transition_duration': avg_duration,
            'most_common_pathways': most_common_paths
        }
    
    def _calculate_path_length_distribution(self, temporal_data: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—è·¯å¾„é•¿åº¦åˆ†å¸ƒ"""
        path_lengths = []
        
        for user_id, user_data in temporal_data.groupby('user_id'):
            # è®¡ç®—æ¯ä¸ªç”¨æˆ·ç»å†çš„ä¸åŒè§’è‰²æ•°é‡
            unique_roles = user_data['role'].nunique()
            path_lengths.append(unique_roles)
        
        if path_lengths:
            distribution = {
                'mean_length': np.mean(path_lengths),
                'std_length': np.std(path_lengths),
                'min_length': min(path_lengths),
                'max_length': max(path_lengths),
                'length_counts': dict(Counter(path_lengths))
            }
        else:
            distribution = {
                'mean_length': 0,
                'std_length': 0,
                'min_length': 0,
                'max_length': 0,
                'length_counts': {}
            }
        
        return distribution
    
    def _identify_bridge_roles(self, temporal_data: pd.DataFrame) -> Dict[str, float]:
        """è¯†åˆ«åœ¨è§’è‰²è½¬æ¢ä¸­èµ·æ¡¥æ¢ä½œç”¨çš„è§’è‰²"""
        role_bridge_scores = defaultdict(int)
        total_transitions = 0
        
        for user_id, user_data in temporal_data.groupby('user_id'):
            user_data = user_data.sort_values('time_period')
            roles = user_data['role'].tolist()
            
            # åˆ†æä¸‰å…ƒç»„è½¬æ¢æ¨¡å¼ A -> B -> C
            for i in range(len(roles) - 2):
                role_a, role_b, role_c = roles[i], roles[i + 1], roles[i + 2]
                
                if role_a != role_b and role_b != role_c and role_a != role_c:
                    # Bæ˜¯ä»Aåˆ°Cçš„æ¡¥æ¢è§’è‰²
                    role_bridge_scores[role_b] += 1
                    total_transitions += 1
        
        # è®¡ç®—æ¡¥æ¢å¾—åˆ†ï¼ˆå½’ä¸€åŒ–ï¼‰
        if total_transitions > 0:
            bridge_roles = {
                role: score / total_transitions
                for role, score in role_bridge_scores.items()
            }
        else:
            bridge_roles = {}
        
        return bridge_roles
    
    def _analyze_role_stability(self, temporal_data: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æè§’è‰²ç¨³å®šæ€§"""
        logger.info("åˆ†æè§’è‰²ç¨³å®šæ€§...")
        
        stability_metrics = {}
        
        for role in self.role_mapping.values():
            role_data = temporal_data[temporal_data['role'] == role]
            
            if role_data.empty:
                stability_metrics[role] = {
                    'retention_rate': 0,
                    'avg_tenure': 0,
                    'transition_rate': 0
                }
                continue
            
            # è®¡ç®—ä¿ç•™ç‡ï¼ˆè¿ç»­æ—¶æœŸä¿æŒåŒä¸€è§’è‰²çš„æ¯”ä¾‹ï¼‰
            retention_count = 0
            total_periods = 0
            
            # è®¡ç®—å¹³å‡ä»»æœŸé•¿åº¦
            tenure_lengths = []
            
            for user_id, user_data in role_data.groupby('user_id'):
                user_data = user_data.sort_values('time_period')
                user_all_data = temporal_data[temporal_data['user_id'] == user_id].sort_values('time_period')
                
                # æ‰¾åˆ°è¯¥è§’è‰²çš„è¿ç»­æ—¶æœŸ
                current_tenure = 0
                for _, row in user_all_data.iterrows():
                    if row['role'] == role:
                        current_tenure += 1
                    else:
                        if current_tenure > 0:
                            tenure_lengths.append(current_tenure)
                        current_tenure = 0
                
                if current_tenure > 0:
                    tenure_lengths.append(current_tenure)
                
                # è®¡ç®—ä¿ç•™ç‡
                for i in range(len(user_all_data) - 1):
                    if user_all_data.iloc[i]['role'] == role:
                        total_periods += 1
                        if user_all_data.iloc[i + 1]['role'] == role:
                            retention_count += 1
            
            # è®¡ç®—æŒ‡æ ‡
            retention_rate = retention_count / total_periods if total_periods > 0 else 0
            avg_tenure = np.mean(tenure_lengths) if tenure_lengths else 0
            transition_rate = 1 - retention_rate
            
            stability_metrics[role] = {
                'retention_rate': retention_rate,
                'avg_tenure': avg_tenure,
                'transition_rate': transition_rate,
                'total_observations': total_periods
            }
        
        return stability_metrics
    
    def _identify_transition_triggers(self, temporal_data: pd.DataFrame) -> Dict[str, Any]:
        """è¯†åˆ«è§’è‰²è½¬æ¢çš„è§¦å‘å› ç´ """
        logger.info("è¯†åˆ«è½¬æ¢è§¦å‘å› ç´ ...")
        
        triggers = {}
        
        if self.activity_data is not None:
            # åˆ†ææ´»åŠ¨æ•°æ®ä¸è§’è‰²è½¬æ¢çš„å…³ç³»
            activity_triggers = self._analyze_activity_based_triggers(temporal_data)
            triggers['activity_based'] = activity_triggers
        
        # åˆ†ææ—¶é—´åŸºç¡€çš„è§¦å‘æ¨¡å¼
        temporal_triggers = self._analyze_temporal_triggers(temporal_data)
        triggers['temporal_patterns'] = temporal_triggers
        
        # åˆ†æè§’è‰²åºåˆ—æ¨¡å¼
        sequence_triggers = self._analyze_sequence_triggers(temporal_data)
        triggers['sequence_patterns'] = sequence_triggers
        
        return triggers
    
    def _analyze_activity_based_triggers(self, temporal_data: pd.DataFrame) -> Dict[str, Any]:
        """åŸºäºæ´»åŠ¨æ•°æ®åˆ†æè½¬æ¢è§¦å‘å› ç´ """
        # è¿™é‡Œéœ€è¦æ›´å¤æ‚çš„æ´»åŠ¨æ•°æ®åˆ†æ
        # ç®€åŒ–ç‰ˆæœ¬ï¼šåˆ†ææ´»åŠ¨é‡å˜åŒ–ä¸è§’è‰²è½¬æ¢çš„å…³ç³»
        
        activity_triggers = {}
        
        # åˆå¹¶æ—¶é—´æ•°æ®å’Œæ´»åŠ¨æ•°æ®
        if 'timestamp' in self.activity_data.columns and 'user_id' in self.activity_data.columns:
            # æŒ‰æœˆèšåˆæ´»åŠ¨æ•°æ®
            self.activity_data['month'] = pd.to_datetime(self.activity_data['timestamp']).dt.to_period('M')
            monthly_activity = self.activity_data.groupby(['user_id', 'month']).size().reset_index(name='activity_count')
            
            # åˆ†ææ´»åŠ¨é‡å˜åŒ–ä¸è§’è‰²è½¬æ¢çš„ç›¸å…³æ€§
            transition_users = []
            for user_id, user_data in temporal_data.groupby('user_id'):
                user_data = user_data.sort_values('time_period')
                for i in range(len(user_data) - 1):
                    if user_data.iloc[i]['role'] != user_data.iloc[i + 1]['role']:
                        transition_users.append({
                            'user_id': user_id,
                            'transition_period': user_data.iloc[i + 1]['time_period'],
                            'from_role': user_data.iloc[i]['role'],
                            'to_role': user_data.iloc[i + 1]['role']
                        })
            
            activity_triggers['transition_count'] = len(transition_users)
            activity_triggers['users_with_transitions'] = len(set(u['user_id'] for u in transition_users))
        
        return activity_triggers
    
    def _analyze_temporal_triggers(self, temporal_data: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†ææ—¶é—´åŸºç¡€çš„è§¦å‘æ¨¡å¼"""
        temporal_triggers = {}
        
        # åˆ†æè½¬æ¢å‘ç”Ÿçš„æ—¶é—´æ¨¡å¼
        transition_times = []
        
        for user_id, user_data in temporal_data.groupby('user_id'):
            user_data = user_data.sort_values('time_period')
            for i in range(len(user_data) - 1):
                if user_data.iloc[i]['role'] != user_data.iloc[i + 1]['role']:
                    transition_times.append(user_data.iloc[i + 1]['time_period'])
        
        if transition_times:
            # æŒ‰æœˆä»½åˆ†æè½¬æ¢é¢‘ç‡
            transition_months = [t.month for t in transition_times]
            month_distribution = dict(Counter(transition_months))
            
            # æŒ‰å­£åº¦åˆ†æ
            transition_quarters = [f"Q{(t.month-1)//3 + 1}" for t in transition_times]
            quarter_distribution = dict(Counter(transition_quarters))
            
            temporal_triggers['monthly_distribution'] = month_distribution
            temporal_triggers['quarterly_distribution'] = quarter_distribution
            temporal_triggers['total_transitions'] = len(transition_times)
        
        return temporal_triggers
    
    def _analyze_sequence_triggers(self, temporal_data: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æåºåˆ—æ¨¡å¼è§¦å‘å› ç´ """
        sequence_triggers = {}
        
        # åˆ†æå¯¼è‡´è½¬æ¢çš„å‰ç½®è§’è‰²æ¨¡å¼
        pre_transition_patterns = defaultdict(int)
        
        for user_id, user_data in temporal_data.groupby('user_id'):
            user_data = user_data.sort_values('time_period')
            roles = user_data['role'].tolist()
            
            for i in range(2, len(roles)):
                if roles[i-2] != roles[i-1] or roles[i-1] != roles[i]:
                    # å‘ç”Ÿäº†è½¬æ¢ï¼Œè®°å½•å‰ç½®æ¨¡å¼
                    pattern = f"{roles[i-2]} -> {roles[i-1]} -> {roles[i]}"
                    pre_transition_patterns[pattern] += 1
        
        # æ‰¾åˆ°æœ€å¸¸è§çš„è½¬æ¢æ¨¡å¼
        common_patterns = sorted(pre_transition_patterns.items(), key=lambda x: x[1], reverse=True)[:10]
        
        sequence_triggers['common_transition_patterns'] = [
            {'pattern': pattern, 'count': count} for pattern, count in common_patterns
        ]
        
        return sequence_triggers
    
    def _predict_role_transitions(self, temporal_data: pd.DataFrame) -> Dict[str, Any]:
        """é¢„æµ‹è§’è‰²è½¬æ¢"""
        logger.info("æ„å»ºè§’è‰²è½¬æ¢é¢„æµ‹æ¨¡å‹...")
        
        prediction_results = {}
        
        try:
            # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
            features, labels = self._prepare_prediction_data(temporal_data)
            
            if len(features) > 10:  # éœ€è¦è¶³å¤Ÿçš„æ•°æ®
                # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
                X_train, X_test, y_train, y_test = train_test_split(
                    features, labels, test_size=0.3, random_state=42, stratify=labels
                )
                
                # è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
                model = RandomForestClassifier(n_estimators=100, random_state=42)
                model.fit(X_train, y_train)
                
                # é¢„æµ‹å’Œè¯„ä¼°
                y_pred = model.predict(X_test)
                
                # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
                class_report = classification_report(y_test, y_pred, output_dict=True)
                
                # ç‰¹å¾é‡è¦æ€§
                feature_importance = dict(zip(
                    [f'feature_{i}' for i in range(len(features[0]))],
                    model.feature_importances_
                ))
                
                prediction_results = {
                    'model_accuracy': model.score(X_test, y_test),
                    'classification_report': class_report,
                    'feature_importance': feature_importance,
                    'training_samples': len(X_train),
                    'test_samples': len(X_test)
                }
                
                # å­˜å‚¨æ¨¡å‹
                self.prediction_model = model
                
            else:
                prediction_results['error'] = 'Insufficient data for prediction model'
        
        except Exception as e:
            logger.error(f"é¢„æµ‹æ¨¡å‹æ„å»ºå¤±è´¥: {e}")
            prediction_results['error'] = str(e)
        
        return prediction_results
    
    def _prepare_prediction_data(self, temporal_data: pd.DataFrame) -> Tuple[List, List]:
        """å‡†å¤‡é¢„æµ‹æ¨¡å‹çš„ç‰¹å¾å’Œæ ‡ç­¾æ•°æ®"""
        features = []
        labels = []
        
        for user_id, user_data in temporal_data.groupby('user_id'):
            user_data = user_data.sort_values('time_period')
            
            for i in range(len(user_data) - 1):
                current_row = user_data.iloc[i]
                next_row = user_data.iloc[i + 1]
                
                # ç‰¹å¾ï¼šå½“å‰è§’è‰²ã€æ—¶æœŸã€ç”¨æˆ·ç»Ÿè®¡ä¿¡æ¯ç­‰
                feature_vector = [
                    list(self.role_mapping.values()).index(current_row['role']),  # å½“å‰è§’è‰²ç¼–ç 
                    i,  # æ—¶æœŸç´¢å¼•
                    len(user_data),  # ç”¨æˆ·æ€»æ´»è·ƒæ—¶æœŸ
                ]
                
                # æ·»åŠ ç”¨æˆ·ç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if user_id in self.user_roles_df['user_id'].values:
                    user_features = self.user_roles_df[self.user_roles_df['user_id'] == user_id].iloc[0]
                    feature_vector.extend([
                        user_features.get('pr_count', 0),
                        user_features.get('issue_count', 0),
                        user_features.get('star_count', 0),
                        user_features.get('code_focus_ratio', 0),
                        user_features.get('interaction_diversity', 0)
                    ])
                else:
                    feature_vector.extend([0, 0, 0, 0, 0])
                
                features.append(feature_vector)
                labels.append(list(self.role_mapping.values()).index(next_row['role']))
        
        return features, labels
    
    def _calculate_transition_summary(self, 
                                    transition_matrix: pd.DataFrame,
                                    stability_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—è½¬æ¢åˆ†ææ‘˜è¦ç»Ÿè®¡"""
        summary = {}
        
        if not transition_matrix.empty:
            # è½¬æ¢çŸ©é˜µç»Ÿè®¡
            summary['total_possible_transitions'] = len(transition_matrix) ** 2
            summary['observed_transitions'] = (transition_matrix > 0).sum().sum()
            summary['transition_diversity'] = summary['observed_transitions'] / summary['total_possible_transitions']
            
            # æœ€ç¨³å®šçš„è§’è‰²ï¼ˆå¯¹è§’çº¿å…ƒç´ æœ€å¤§ï¼‰
            diagonal_values = np.diag(transition_matrix.values)
            most_stable_idx = np.argmax(diagonal_values)
            summary['most_stable_role'] = transition_matrix.index[most_stable_idx]
            summary['highest_stability_score'] = diagonal_values[most_stable_idx]
            
            # æœ€ä¸ç¨³å®šçš„è§’è‰²
            least_stable_idx = np.argmin(diagonal_values)
            summary['least_stable_role'] = transition_matrix.index[least_stable_idx]
            summary['lowest_stability_score'] = diagonal_values[least_stable_idx]
        
        # ç¨³å®šæ€§åˆ†æç»Ÿè®¡
        if stability_analysis:
            retention_rates = [metrics['retention_rate'] for metrics in stability_analysis.values()]
            if retention_rates:
                summary['avg_retention_rate'] = np.mean(retention_rates)
                summary['std_retention_rate'] = np.std(retention_rates)
        
        return summary
    
    def _save_transition_results(self, results: Dict[str, Any]):
        """ä¿å­˜è½¬æ¢åˆ†æç»“æœ"""
        # ä¿å­˜è½¬æ¢çŸ©é˜µ
        if 'transition_matrix' in results and not results['transition_matrix'].empty:
            matrix_path = ANALYSIS_OUTPUT_DIR / "role_transition_matrix.csv"
            results['transition_matrix'].to_csv(matrix_path, encoding='utf-8-sig')
            logger.info(f"è½¬æ¢çŸ©é˜µå·²ä¿å­˜è‡³: {matrix_path}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        import json
        
        # å¤„ç†ä¸èƒ½JSONåºåˆ—åŒ–çš„å¯¹è±¡
        json_results = {}
        for key, value in results.items():
            if key == 'transition_matrix':
                json_results[key] = value.to_dict() if hasattr(value, 'to_dict') else str(value)
            elif isinstance(value, dict):
                json_results[key] = value
            else:
                json_results[key] = str(value)
        
        results_path = ANALYSIS_OUTPUT_DIR / "role_transition_analysis.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(json_results, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"è§’è‰²è½¬æ¢åˆ†æç»“æœå·²ä¿å­˜è‡³: {results_path}")
    
    def visualize_transition_patterns(self, results: Dict[str, Any]):
        """å¯è§†åŒ–è§’è‰²è½¬æ¢æ¨¡å¼"""
        plt.style.use(VISUALIZATION_CONFIG["style"])
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. è½¬æ¢çŸ©é˜µçƒ­åŠ›å›¾
        if 'transition_matrix' in results and not results['transition_matrix'].empty:
            transition_matrix = results['transition_matrix']
            sns.heatmap(transition_matrix, annot=True, cmap='Blues', ax=axes[0,0], fmt='.3f')
            axes[0,0].set_title('Role Transition Matrix', fontsize=VISUALIZATION_CONFIG["title_font_size"])
            axes[0,0].set_xlabel('To Role')
            axes[0,0].set_ylabel('From Role')
        
        # 2. è§’è‰²ç¨³å®šæ€§
        if 'stability_analysis' in results:
            stability = results['stability_analysis']
            roles = list(stability.keys())
            retention_rates = [stability[role]['retention_rate'] for role in roles]
            
            bars = axes[0,1].bar(range(len(roles)), retention_rates, color='lightcoral', alpha=0.7)
            axes[0,1].set_title('Role Stability (Retention Rates)', fontsize=VISUALIZATION_CONFIG["title_font_size"])
            axes[0,1].set_xlabel('Roles')
            axes[0,1].set_ylabel('Retention Rate')
            axes[0,1].set_xticks(range(len(roles)))
            axes[0,1].set_xticklabels(roles, rotation=45, ha='right')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, rate in zip(bars, retention_rates):
                axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                              f'{rate:.2f}', ha='center', va='bottom')
        
        # 3. è·¯å¾„é•¿åº¦åˆ†å¸ƒ
        if 'pathway_analysis' in results and 'path_length_distribution' in results['pathway_analysis']:
            path_dist = results['pathway_analysis']['path_length_distribution']
            if 'length_counts' in path_dist and path_dist['length_counts']:
                lengths = list(path_dist['length_counts'].keys())
                counts = list(path_dist['length_counts'].values())
                
                axes[1,0].bar(lengths, counts, color='lightgreen', alpha=0.7)
                axes[1,0].set_title('Path Length Distribution', fontsize=VISUALIZATION_CONFIG["title_font_size"])
                axes[1,0].set_xlabel('Number of Different Roles')
                axes[1,0].set_ylabel('Number of Users')
        
        # 4. å¸¸è§è½¬æ¢åºåˆ—
        if 'pathway_analysis' in results and 'common_sequences' in results['pathway_analysis']:
            common_seqs = results['pathway_analysis']['common_sequences'][:10]  # å‰10ä¸ª
            if common_seqs:
                seq_labels = [' -> '.join(seq['sequence']) for seq in common_seqs]
                seq_counts = [seq['count'] for seq in common_seqs]
                
                y_pos = np.arange(len(seq_labels))
                axes[1,1].barh(y_pos, seq_counts, color='gold', alpha=0.7)
                axes[1,1].set_title('Most Common Transition Sequences', fontsize=VISUALIZATION_CONFIG["title_font_size"])
                axes[1,1].set_xlabel('Frequency')
                axes[1,1].set_yticks(y_pos)
                axes[1,1].set_yticklabels(seq_labels, fontsize=8)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        save_path = ANALYSIS_OUTPUT_DIR / "role_transition_visualization.png"
        plt.savefig(save_path, dpi=VISUALIZATION_CONFIG["dpi"], bbox_inches='tight')
        logger.info(f"è§’è‰²è½¬æ¢å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")
        
        plt.show()
    
    def generate_transition_report(self, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆè§’è‰²è½¬æ¢åˆ†ææŠ¥å‘Š"""
        report_lines = [
            "=" * 60,
            "è§’è‰²è½¬æ¢è·¯å¾„åˆ†ææŠ¥å‘Š",
            "Role Transition Analysis Report", 
            "=" * 60,
            ""
        ]
        
        # æ‘˜è¦ç»Ÿè®¡
        if 'summary_statistics' in results:
            summary = results['summary_statistics']
            report_lines.extend([
                "ğŸ“Š æ‘˜è¦ç»Ÿè®¡ (Summary Statistics):",
                f"  - è§‚å¯Ÿåˆ°çš„è½¬æ¢ç±»å‹: {summary.get('observed_transitions', 'N/A')}/{summary.get('total_possible_transitions', 'N/A')}",
                f"  - è½¬æ¢å¤šæ ·æ€§: {summary.get('transition_diversity', 'N/A'):.3f}",
                f"  - æœ€ç¨³å®šè§’è‰²: {summary.get('most_stable_role', 'N/A')} (ç¨³å®šæ€§: {summary.get('highest_stability_score', 'N/A'):.3f})",
                f"  - æœ€ä¸ç¨³å®šè§’è‰²: {summary.get('least_stable_role', 'N/A')} (ç¨³å®šæ€§: {summary.get('lowest_stability_score', 'N/A'):.3f})",
                f"  - å¹³å‡ä¿ç•™ç‡: {summary.get('avg_retention_rate', 'N/A'):.3f}",
                ""
            ])
        
        # è·¯å¾„åˆ†æç»“æœ
        if 'pathway_analysis' in results:
            pathway = results['pathway_analysis']
            report_lines.extend([
                "ğŸ›¤ï¸  è½¬æ¢è·¯å¾„åˆ†æ (Pathway Analysis):",
                ""
            ])
            
            if 'novice_expert_paths' in pathway:
                novice_expert = pathway['novice_expert_paths']
                report_lines.extend([
                    f"æ–°æ‰‹åˆ°ä¸“å®¶è·¯å¾„:",
                    f"  - å‘ç°è·¯å¾„æ•°: {novice_expert.get('total_pathways_found', 0)}",
                    f"  - å¹³å‡è·¯å¾„é•¿åº¦: {novice_expert.get('average_pathway_length', 0):.1f}",
                    f"  - å¹³å‡è½¬æ¢æ—¶é•¿: {novice_expert.get('average_transition_duration', 0):.1f} ä¸ªæ—¶æœŸ",
                    ""
                ])
                
                if 'most_common_pathways' in novice_expert and novice_expert['most_common_pathways']:
                    report_lines.append("æœ€å¸¸è§çš„æ–°æ‰‹â†’ä¸“å®¶è·¯å¾„:")
                    for path in novice_expert['most_common_pathways'][:5]:
                        report_lines.append(f"  - {' -> '.join(path['pathway'])} ({path['count']} æ¬¡)")
                    report_lines.append("")
            
            if 'bridge_roles' in pathway:
                bridge_roles = pathway['bridge_roles']
                if bridge_roles:
                    sorted_bridges = sorted(bridge_roles.items(), key=lambda x: x[1], reverse=True)
                    report_lines.extend([
                        "æ¡¥æ¢è§’è‰² (Bridge Roles):",
                        *(f"  - {role}: {score:.3f}" for role, score in sorted_bridges[:5]),
                        ""
                    ])
        
        # é¢„æµ‹ç»“æœ
        if 'prediction_results' in results and 'model_accuracy' in results['prediction_results']:
            pred_results = results['prediction_results']
            report_lines.extend([
                "ğŸ”® è½¬æ¢é¢„æµ‹æ¨¡å‹ (Prediction Model):",
                f"  - æ¨¡å‹å‡†ç¡®ç‡: {pred_results.get('model_accuracy', 'N/A'):.3f}",
                f"  - è®­ç»ƒæ ·æœ¬æ•°: {pred_results.get('training_samples', 'N/A')}",
                f"  - æµ‹è¯•æ ·æœ¬æ•°: {pred_results.get('test_samples', 'N/A')}",
                ""
            ])
            
            if 'feature_importance' in pred_results:
                importance = pred_results['feature_importance']
                sorted_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)
                report_lines.extend([
                    "é‡è¦ç‰¹å¾ (Top Features):",
                    *(f"  - {feature}: {importance:.3f}" for feature, importance in sorted_features[:5]),
                    ""
                ])
        
        report_lines.extend([
            "=" * 60,
            f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "=" * 60
        ])
        
        report = "\n".join(report_lines)
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = ANALYSIS_OUTPUT_DIR / "role_transition_report.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"è§’è‰²è½¬æ¢åˆ†ææŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
        
        return report


def main():
    """ä¸»å‡½æ•°å…¥å£"""
    logger.info("è§’è‰²è½¬æ¢è·¯å¾„åˆ†ææ¨¡å—æµ‹è¯•")
    
    print("è§’è‰²è½¬æ¢è·¯å¾„åˆ†æå™¨å·²å®ç°ä»¥ä¸‹åŠŸèƒ½:")
    print("1. è§’è‰²è½¬æ¢çŸ©é˜µè®¡ç®—")
    print("2. è½¬æ¢è·¯å¾„æ¨¡å¼åˆ†æ")
    print("3. æ–°æ‰‹åˆ°ä¸“å®¶è·¯å¾„è¯†åˆ«")
    print("4. è§’è‰²ç¨³å®šæ€§åˆ†æ")
    print("5. è½¬æ¢è§¦å‘å› ç´ è¯†åˆ«")
    print("6. è½¬æ¢é¢„æµ‹æ¨¡å‹")
    print("7. å¯è§†åŒ–å’ŒæŠ¥å‘Šç”Ÿæˆ")
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("analyzer = RoleTransitionAnalyzer(user_roles_df, activity_data)")
    print("results = analyzer.analyze_role_transitions(temporal_data)")
    print("analyzer.visualize_transition_patterns(results)")
    print("report = analyzer.generate_transition_report(results)")


if __name__ == "__main__":
    main()


