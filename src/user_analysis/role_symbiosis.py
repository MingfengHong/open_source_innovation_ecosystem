"""
è§’è‰²å…±ç”Ÿå…³ç³»åˆ†ææ¨¡å—
å®ç°ç”¨æˆ·è§’è‰²é—´çš„ä¾èµ–å…³ç³»é‡åŒ–éªŒè¯ï¼Œæ”¯æŒRQ2ç ”ç©¶é—®é¢˜
"""

import pandas as pd
import numpy as np
import networkx as nx
from typing import Dict, List, Tuple, Optional, Any, Set
import logging
from collections import defaultdict, Counter
from datetime import datetime, timedelta
import matplotlib.pyplot as plt

try:
    import seaborn as sns
except ImportError:
    sns = None
from scipy import stats
from scipy.stats import chi2_contingency, pearsonr, spearmanr
from sklearn.metrics import mutual_info_score
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

from ..utils.logging_config import setup_logger
from config.settings import ANALYSIS_OUTPUT_DIR, VISUALIZATION_CONFIG

# è®¾ç½®æ—¥å¿—
logger = setup_logger(__name__)


class RoleSymbiosisAnalyzer:
    """
    è§’è‰²å…±ç”Ÿå…³ç³»åˆ†æå™¨
    é‡åŒ–éªŒè¯ä¸åŒç”¨æˆ·è§’è‰²ä¹‹é—´çš„ä¾èµ–å…³ç³»å’Œå…±ç”Ÿæ¨¡å¼
    """
    
    def __init__(self, 
                 user_roles_df: pd.DataFrame,
                 network_graph: nx.Graph = None,
                 time_window_months: int = 3):
        """
        åˆå§‹åŒ–è§’è‰²å…±ç”Ÿåˆ†æå™¨
        
        Args:
            user_roles_df: ç”¨æˆ·è§’è‰²æ•°æ®æ¡†ï¼ŒåŒ…å«user_id, clusterç­‰å­—æ®µ
            network_graph: ç½‘ç»œå›¾ï¼ˆå¯é€‰ï¼Œç”¨äºç½‘ç»œä½ç½®åˆ†æï¼‰
            time_window_months: åˆ†ææ—¶é—´çª—å£ï¼ˆæœˆï¼‰
        """
        self.user_roles_df = user_roles_df.copy()
        self.network_graph = network_graph
        self.time_window_months = time_window_months
        
        # å®šä¹‰è§’è‰²æ˜ å°„ï¼ˆåŸºäºèšç±»ç»“æœï¼‰
        self.role_mapping = self._create_role_mapping()
        
        # å­˜å‚¨åˆ†æç»“æœ
        self.symbiosis_results = {}
        self.dependency_matrix = None
        self.transition_probabilities = None
        
        logger.info(f"åˆå§‹åŒ–è§’è‰²å…±ç”Ÿåˆ†æå™¨: {len(user_roles_df)} ä¸ªç”¨æˆ·, {len(self.role_mapping)} ç§è§’è‰²")
    
    def _create_role_mapping(self) -> Dict[int, str]:
        """
        åŸºäºèšç±»ç»“æœå’Œè¡Œä¸ºç‰¹å¾åˆ›å»ºè§’è‰²æ˜ å°„
        
        Returns:
            Dict[int, str]: èšç±»IDåˆ°è§’è‰²åç§°çš„æ˜ å°„
        """
        role_mapping = {}
        
        if 'cluster' not in self.user_roles_df.columns:
            logger.warning("ç”¨æˆ·æ•°æ®ä¸­ç¼ºå°‘clusteråˆ—ï¼Œä½¿ç”¨é»˜è®¤è§’è‰²åˆ†é…")
            # åˆ›å»ºé»˜è®¤è§’è‰²åˆ†é…
            unique_users = len(self.user_roles_df)
            for i in range(min(6, unique_users)):  # æœ€å¤š6ç§è§’è‰²
                role_mapping[i] = f"role_{i}"
            return role_mapping
        
        # åˆ†ææ¯ä¸ªèšç±»çš„ç‰¹å¾æ¥ç¡®å®šè§’è‰²åç§°
        cluster_stats = self.user_roles_df.groupby('cluster').agg({
            'pr_count': 'mean',
            'issue_count': 'mean', 
            'star_count': 'mean',
            'repo_count': 'mean',
            'code_focus_ratio': 'mean',
            'interaction_diversity': 'mean'
        }).fillna(0)
        
        for cluster_id, stats in cluster_stats.iterrows():
            # åŸºäºç‰¹å¾æ¨¡å¼ç¡®å®šè§’è‰²åç§°
            if stats['code_focus_ratio'] > 0.7 and stats['pr_count'] > stats.mean()['pr_count']:
                role_name = "core_developer"  # æ ¸å¿ƒå¼€å‘è€…
            elif stats['interaction_diversity'] > 3 and stats['issue_count'] > stats.mean()['issue_count']:
                role_name = "community_facilitator"  # ç¤¾åŒºä¿ƒè¿›è€…/å¸ƒé“è€…
            elif stats['repo_count'] > stats.mean()['repo_count'] and stats['code_focus_ratio'] > 0.5:
                role_name = "architect"  # æ¶æ„å¸ˆ/é¡¹ç›®åˆ›å»ºè€…
            elif stats['issue_count'] > stats['pr_count'] and stats['interaction_diversity'] > 2:
                role_name = "problem_solver"  # é—®é¢˜è§£å†³è€…
            elif stats['star_count'] > stats.mean()['star_count'] and stats['pr_count'] < stats.mean()['pr_count']:
                role_name = "observer"  # è§‚å¯Ÿè€…/å­¦ä¹ è€…
            else:
                role_name = "casual_contributor"  # å¶ç„¶è´¡çŒ®è€…
            
            role_mapping[cluster_id] = role_name
        
        logger.info(f"è§’è‰²æ˜ å°„: {role_mapping}")
        return role_mapping
    
    def analyze_role_dependencies(self, 
                                activity_data: pd.DataFrame) -> Dict[str, Any]:
        """
        åˆ†æè§’è‰²é—´çš„ä¾èµ–å…³ç³»
        
        Args:
            activity_data: æ´»åŠ¨æ•°æ®ï¼ŒåŒ…å«user_id, activity_type, target_id, timestampç­‰
            
        Returns:
            Dict[str, Any]: ä¾èµ–å…³ç³»åˆ†æç»“æœ
        """
        logger.info("åˆ†æè§’è‰²é—´çš„ä¾èµ–å…³ç³»...")
        
        # å‡†å¤‡æ•°æ®
        activity_with_roles = self._prepare_activity_data(activity_data)
        
        if activity_with_roles.empty:
            logger.error("æ´»åŠ¨æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œä¾èµ–å…³ç³»åˆ†æ")
            return {}
        
        # 1. è®¡ç®—è§’è‰²é—´çš„åä½œé¢‘ç‡
        collaboration_matrix = self._calculate_collaboration_matrix(activity_with_roles)
        
        # 2. åˆ†æè§’è‰²é—´çš„æ—¶é—´ä¾èµ–å…³ç³»
        temporal_dependencies = self._analyze_temporal_dependencies(activity_with_roles)
        
        # 3. è®¡ç®—è§’è‰²äº’è¡¥æ€§æŒ‡æ•°
        complementarity_scores = self._calculate_role_complementarity(activity_with_roles)
        
        # 4. åˆ†æçŸ¥è¯†æµåŠ¨æ¨¡å¼
        knowledge_flow = self._analyze_knowledge_flow_patterns(activity_with_roles)
        
        # 5. éªŒè¯ç‰¹å®šçš„å…±ç”Ÿå‡è®¾
        symbiosis_hypotheses = self._validate_symbiosis_hypotheses(activity_with_roles)
        
        results = {
            'collaboration_matrix': collaboration_matrix,
            'temporal_dependencies': temporal_dependencies,
            'complementarity_scores': complementarity_scores,
            'knowledge_flow': knowledge_flow,
            'symbiosis_hypotheses': symbiosis_hypotheses,
            'summary_statistics': self._calculate_dependency_summary(
                collaboration_matrix, temporal_dependencies, complementarity_scores
            )
        }
        
        self.symbiosis_results = results
        
        # ä¿å­˜ç»“æœ
        self._save_symbiosis_results(results)
        
        return results
    
    def _prepare_activity_data(self, activity_data: pd.DataFrame) -> pd.DataFrame:
        """å‡†å¤‡åˆ†æç”¨çš„æ´»åŠ¨æ•°æ®"""
        # åˆå¹¶ç”¨æˆ·è§’è‰²ä¿¡æ¯
        activity_with_roles = pd.merge(
            activity_data, 
            self.user_roles_df[['user_id', 'cluster']], 
            on='user_id', 
            how='left'
        )
        
        # æ·»åŠ è§’è‰²åç§°
        activity_with_roles['role'] = activity_with_roles['cluster'].map(self.role_mapping)
        
        # è¿‡æ»¤æ‰æ²¡æœ‰è§’è‰²ä¿¡æ¯çš„æ•°æ®
        activity_with_roles = activity_with_roles.dropna(subset=['role'])
        
        # è½¬æ¢æ—¶é—´æˆ³
        if 'timestamp' in activity_with_roles.columns:
            activity_with_roles['timestamp'] = pd.to_datetime(activity_with_roles['timestamp'])
        
        logger.info(f"å‡†å¤‡æ´»åŠ¨æ•°æ®: {len(activity_with_roles)} æ¡è®°å½•, {activity_with_roles['role'].nunique()} ç§è§’è‰²")
        
        return activity_with_roles
    
    def _calculate_collaboration_matrix(self, activity_data: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—è§’è‰²é—´çš„åä½œé¢‘ç‡çŸ©é˜µ"""
        logger.info("è®¡ç®—è§’è‰²åä½œçŸ©é˜µ...")
        
        roles = list(self.role_mapping.values())
        collaboration_matrix = pd.DataFrame(0, index=roles, columns=roles)
        
        # åŸºäºå…±åŒå‚ä¸çš„é¡¹ç›®/ä»“åº“è®¡ç®—åä½œ
        if 'target_id' in activity_data.columns:
            # æŒ‰é¡¹ç›®åˆ†ç»„ï¼Œç»Ÿè®¡è§’è‰²å…±ç°
            project_groups = activity_data.groupby('target_id')
            
            for project_id, group in project_groups:
                project_roles = group['role'].value_counts()
                
                # è®¡ç®—è§’è‰²å¯¹ä¹‹é—´çš„åä½œå¼ºåº¦
                for role1 in project_roles.index:
                    for role2 in project_roles.index:
                        if role1 != role2:
                            # åä½œå¼ºåº¦ = min(role1_count, role2_count) * é¡¹ç›®é‡è¦æ€§æƒé‡
                            weight = min(project_roles[role1], project_roles[role2])
                            collaboration_matrix.loc[role1, role2] += weight
        
        # å½’ä¸€åŒ–
        for role in roles:
            total = collaboration_matrix.loc[role].sum()
            if total > 0:
                collaboration_matrix.loc[role] = collaboration_matrix.loc[role] / total
        
        return collaboration_matrix
    
    def _analyze_temporal_dependencies(self, activity_data: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æè§’è‰²é—´çš„æ—¶é—´ä¾èµ–å…³ç³»"""
        logger.info("åˆ†ææ—¶é—´ä¾èµ–å…³ç³»...")
        
        if 'timestamp' not in activity_data.columns:
            logger.warning("ç¼ºå°‘æ—¶é—´æˆ³ä¿¡æ¯ï¼Œè·³è¿‡æ—¶é—´ä¾èµ–åˆ†æ")
            return {}
        
        temporal_deps = {}
        roles = list(self.role_mapping.values())
        
        # æŒ‰æ—¶é—´çª—å£åˆ†æè§’è‰²æ´»åŠ¨çš„å…ˆåå…³ç³»
        activity_data = activity_data.sort_values('timestamp')
        
        # è®¡ç®—è§’è‰²æ´»åŠ¨çš„æ—¶é—´åºåˆ—
        role_activity_series = {}
        for role in roles:
            role_data = activity_data[activity_data['role'] == role]
            if not role_data.empty:
                # æŒ‰æœˆèšåˆæ´»åŠ¨æ•°é‡
                monthly_activity = role_data.set_index('timestamp').resample('M').size()
                role_activity_series[role] = monthly_activity.fillna(0)
        
        # è®¡ç®—è§’è‰²é—´çš„æ ¼å…°æ°å› æœå…³ç³»ï¼ˆç®€åŒ–ç‰ˆï¼‰
        granger_results = {}
        for role1 in roles:
            for role2 in roles:
                if role1 != role2 and role1 in role_activity_series and role2 in role_activity_series:
                    # å¯¹é½æ—¶é—´åºåˆ—
                    series1 = role_activity_series[role1]
                    series2 = role_activity_series[role2]
                    
                    if len(series1) > 3 and len(series2) > 3:
                        # è®¡ç®—æ»åç›¸å…³æ€§
                        lag_correlations = self._calculate_lag_correlations(series1, series2, max_lag=3)
                        granger_results[f"{role1}_to_{role2}"] = lag_correlations
        
        temporal_deps['granger_causality'] = granger_results
        
        # åˆ†ææ´»åŠ¨æ¨¡å¼çš„æ—¶é—´äº’è¡¥æ€§
        complementarity_patterns = self._analyze_temporal_complementarity(role_activity_series)
        temporal_deps['complementarity_patterns'] = complementarity_patterns
        
        return temporal_deps
    
    def _calculate_lag_correlations(self, series1: pd.Series, series2: pd.Series, max_lag: int = 3) -> Dict[int, float]:
        """è®¡ç®—æ»åç›¸å…³æ€§"""
        lag_correlations = {}
        
        # å¯¹é½ç´¢å¼•
        common_index = series1.index.intersection(series2.index)
        if len(common_index) < 4:
            return {}
        
        s1_aligned = series1.reindex(common_index).fillna(0)
        s2_aligned = series2.reindex(common_index).fillna(0)
        
        for lag in range(0, max_lag + 1):
            if lag == 0:
                # åŒæœŸç›¸å…³æ€§
                if len(s1_aligned) > 1 and s1_aligned.std() > 0 and s2_aligned.std() > 0:
                    corr, p_value = pearsonr(s1_aligned, s2_aligned)
                    lag_correlations[lag] = {'correlation': corr, 'p_value': p_value}
            else:
                # æ»åç›¸å…³æ€§
                if len(s1_aligned) > lag + 1:
                    s1_lagged = s1_aligned[:-lag]
                    s2_current = s2_aligned[lag:]
                    
                    if len(s1_lagged) > 1 and s1_lagged.std() > 0 and s2_current.std() > 0:
                        corr, p_value = pearsonr(s1_lagged, s2_current)
                        lag_correlations[lag] = {'correlation': corr, 'p_value': p_value}
        
        return lag_correlations
    
    def _analyze_temporal_complementarity(self, role_activity_series: Dict[str, pd.Series]) -> Dict[str, float]:
        """åˆ†æè§’è‰²æ´»åŠ¨çš„æ—¶é—´äº’è¡¥æ€§"""
        complementarity = {}
        roles = list(role_activity_series.keys())
        
        for i, role1 in enumerate(roles):
            for role2 in roles[i+1:]:
                if role1 in role_activity_series and role2 in role_activity_series:
                    series1 = role_activity_series[role1]
                    series2 = role_activity_series[role2]
                    
                    # è®¡ç®—æ´»åŠ¨æ—¶é—´çš„äº’è¡¥æ€§ï¼ˆè´Ÿç›¸å…³è¡¨ç¤ºäº’è¡¥ï¼‰
                    common_index = series1.index.intersection(series2.index)
                    if len(common_index) > 3:
                        s1 = series1.reindex(common_index).fillna(0)
                        s2 = series2.reindex(common_index).fillna(0)
                        
                        if s1.std() > 0 and s2.std() > 0:
                            corr, _ = pearsonr(s1, s2)
                            # äº’è¡¥æ€§ = 1 - |ç›¸å…³ç³»æ•°|ï¼Œå€¼è¶Šå¤§è¡¨ç¤ºè¶Šäº’è¡¥
                            complementarity[f"{role1}_{role2}"] = 1 - abs(corr)
        
        return complementarity
    
    def _calculate_role_complementarity(self, activity_data: pd.DataFrame) -> Dict[str, float]:
        """è®¡ç®—è§’è‰²äº’è¡¥æ€§æŒ‡æ•°"""
        logger.info("è®¡ç®—è§’è‰²äº’è¡¥æ€§æŒ‡æ•°...")
        
        complementarity_scores = {}
        roles = list(self.role_mapping.values())
        
        # åŸºäºæ´»åŠ¨ç±»å‹çš„äº’è¡¥æ€§åˆ†æ
        if 'activity_type' in activity_data.columns:
            # è®¡ç®—æ¯ç§è§’è‰²åœ¨ä¸åŒæ´»åŠ¨ç±»å‹ä¸Šçš„åˆ†å¸ƒ
            role_activity_distribution = pd.crosstab(
                activity_data['role'], 
                activity_data['activity_type'], 
                normalize='index'
            )
            
            # è®¡ç®—è§’è‰²é—´çš„Jensen-Shannonæ•£åº¦ï¼ˆè¡¡é‡åˆ†å¸ƒå·®å¼‚ï¼‰
            for i, role1 in enumerate(roles):
                for role2 in roles[i+1:]:
                    if role1 in role_activity_distribution.index and role2 in role_activity_distribution.index:
                        dist1 = role_activity_distribution.loc[role1].values
                        dist2 = role_activity_distribution.loc[role2].values
                        
                        # Jensen-Shannonæ•£åº¦
                        js_divergence = self._jensen_shannon_divergence(dist1, dist2)
                        # äº’è¡¥æ€§å¾—åˆ†ï¼šæ•£åº¦è¶Šå¤§ï¼Œäº’è¡¥æ€§è¶Šå¼º
                        complementarity_scores[f"{role1}_{role2}"] = js_divergence
        
        return complementarity_scores
    
    def _jensen_shannon_divergence(self, p: np.ndarray, q: np.ndarray) -> float:
        """è®¡ç®—Jensen-Shannonæ•£åº¦"""
        # ç¡®ä¿æ¦‚ç‡åˆ†å¸ƒå½’ä¸€åŒ–
        p = p / p.sum() if p.sum() > 0 else p
        q = q / q.sum() if q.sum() > 0 else q
        
        # è®¡ç®—å¹³å‡åˆ†å¸ƒ
        m = (p + q) / 2
        
        # é¿å…log(0)
        p = np.where(p == 0, 1e-10, p)
        q = np.where(q == 0, 1e-10, q) 
        m = np.where(m == 0, 1e-10, m)
        
        # Jensen-Shannonæ•£åº¦
        js_div = 0.5 * np.sum(p * np.log(p / m)) + 0.5 * np.sum(q * np.log(q / m))
        
        return js_div
    
    def _analyze_knowledge_flow_patterns(self, activity_data: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æçŸ¥è¯†æµåŠ¨æ¨¡å¼"""
        logger.info("åˆ†æçŸ¥è¯†æµåŠ¨æ¨¡å¼...")
        
        knowledge_flow = {}
        
        # åŸºäºæ–‡æ¡£è´¡çŒ®å’Œä»£ç è´¡çŒ®çš„çŸ¥è¯†æµåŠ¨åˆ†æ
        if 'activity_type' in activity_data.columns:
            doc_activities = activity_data[activity_data['activity_type'].str.contains('doc|documentation', case=False, na=False)]
            code_activities = activity_data[activity_data['activity_type'].str.contains('code|pr', case=False, na=False)]
            
            # åˆ†ææ–‡æ¡£è´¡çŒ®è€…å¯¹ä»£ç è´¡çŒ®è€…çš„å½±å“
            doc_to_code_flow = self._analyze_role_influence_flow(doc_activities, code_activities)
            knowledge_flow['documentation_to_code'] = doc_to_code_flow
            
            # åˆ†æä»£ç è´¡çŒ®è€…å¯¹é—®é¢˜è§£å†³çš„å½±å“
            issue_activities = activity_data[activity_data['activity_type'].str.contains('issue', case=False, na=False)]
            code_to_issue_flow = self._analyze_role_influence_flow(code_activities, issue_activities)
            knowledge_flow['code_to_issue_resolution'] = code_to_issue_flow
        
        return knowledge_flow
    
    def _analyze_role_influence_flow(self, source_activities: pd.DataFrame, target_activities: pd.DataFrame) -> Dict[str, float]:
        """åˆ†æè§’è‰²é—´çš„å½±å“æµåŠ¨"""
        influence_flow = {}
        
        if source_activities.empty or target_activities.empty:
            return influence_flow
        
        # æŒ‰è§’è‰²èšåˆæ´»åŠ¨
        source_by_role = source_activities['role'].value_counts()
        target_by_role = target_activities['role'].value_counts()
        
        # è®¡ç®—å½±å“å¼ºåº¦ï¼ˆåŸºäºæ´»åŠ¨æ•°é‡çš„ç›¸å…³æ€§ï¼‰
        common_roles = set(source_by_role.index).intersection(set(target_by_role.index))
        
        for role in common_roles:
            source_count = source_by_role.get(role, 0)
            target_count = target_by_role.get(role, 0)
            
            if source_count > 0 and target_count > 0:
                # å½±å“å¼ºåº¦ = min(source, target) / max(source, target)
                influence_strength = min(source_count, target_count) / max(source_count, target_count)
                influence_flow[role] = influence_strength
        
        return influence_flow
    
    def _validate_symbiosis_hypotheses(self, activity_data: pd.DataFrame) -> Dict[str, Any]:
        """éªŒè¯ç‰¹å®šçš„å…±ç”Ÿå‡è®¾"""
        logger.info("éªŒè¯è§’è‰²å…±ç”Ÿå‡è®¾...")
        
        hypotheses_results = {}
        
        # å‡è®¾1: å¸ƒé“è€…(community_facilitator)çš„æ–‡æ¡£è´¡çŒ®é™ä½æ¶æ„å¸ˆ(architect)çš„å…¥é—¨é—¨æ§›
        hypothesis1 = self._test_facilitator_architect_symbiosis(activity_data)
        hypotheses_results['facilitator_architect_symbiosis'] = hypothesis1
        
        # å‡è®¾2: æ ¸å¿ƒå¼€å‘è€…(core_developer)ä¸é—®é¢˜è§£å†³è€…(problem_solver)çš„äº’è¡¥å…³ç³»
        hypothesis2 = self._test_developer_solver_complementarity(activity_data)
        hypotheses_results['developer_solver_complementarity'] = hypothesis2
        
        # å‡è®¾3: è§‚å¯Ÿè€…(observer)åˆ°è´¡çŒ®è€…çš„è§’è‰²è½¬æ¢è·¯å¾„
        hypothesis3 = self._test_observer_contributor_transition(activity_data)
        hypotheses_results['observer_contributor_transition'] = hypothesis3
        
        return hypotheses_results
    
    def _test_facilitator_architect_symbiosis(self, activity_data: pd.DataFrame) -> Dict[str, Any]:
        """æµ‹è¯•å¸ƒé“è€…-æ¶æ„å¸ˆå…±ç”Ÿå…³ç³»"""
        result = {'hypothesis': 'Facilitators enable architects through documentation', 'evidence': {}}
        
        facilitators = activity_data[activity_data['role'] == 'community_facilitator']
        architects = activity_data[activity_data['role'] == 'architect']
        
        if facilitators.empty or architects.empty:
            result['conclusion'] = 'Insufficient data'
            return result
        
        # åˆ†ææ–‡æ¡£æ´»åŠ¨å¯¹æ–°é¡¹ç›®åˆ›å»ºçš„æ—¶é—´æ»åå½±å“
        doc_activities = facilitators[facilitators.get('activity_type', '').str.contains('doc', case=False, na=False)]
        project_activities = architects[architects.get('activity_type', '').str.contains('create|repo', case=False, na=False)]
        
        if not doc_activities.empty and not project_activities.empty and 'timestamp' in activity_data.columns:
            # æŒ‰æœˆç»Ÿè®¡æ´»åŠ¨
            doc_monthly = doc_activities.set_index('timestamp').resample('M').size()
            project_monthly = project_activities.set_index('timestamp').resample('M').size()
            
            # è®¡ç®—æ»åç›¸å…³æ€§
            lag_corrs = self._calculate_lag_correlations(doc_monthly, project_monthly, max_lag=2)
            
            result['evidence']['lag_correlations'] = lag_corrs
            
            # å¯»æ‰¾æœ€å¼ºçš„æ­£ç›¸å…³æ»å
            max_corr_lag = max(lag_corrs.keys(), key=lambda k: lag_corrs[k].get('correlation', 0))
            max_corr = lag_corrs[max_corr_lag]['correlation']
            
            if max_corr > 0.3 and lag_corrs[max_corr_lag]['p_value'] < 0.05:
                result['conclusion'] = f'Strong positive evidence (r={max_corr:.3f}, lag={max_corr_lag})'
            elif max_corr > 0.1:
                result['conclusion'] = f'Weak positive evidence (r={max_corr:.3f}, lag={max_corr_lag})'
            else:
                result['conclusion'] = 'No significant evidence'
        else:
            result['conclusion'] = 'Insufficient temporal data'
        
        return result
    
    def _test_developer_solver_complementarity(self, activity_data: pd.DataFrame) -> Dict[str, Any]:
        """æµ‹è¯•å¼€å‘è€…-é—®é¢˜è§£å†³è€…äº’è¡¥å…³ç³»"""
        result = {'hypothesis': 'Developers and problem solvers have complementary activity patterns', 'evidence': {}}
        
        developers = activity_data[activity_data['role'] == 'core_developer']
        solvers = activity_data[activity_data['role'] == 'problem_solver']
        
        if developers.empty or solvers.empty:
            result['conclusion'] = 'Insufficient data'
            return result
        
        # åˆ†ææ´»åŠ¨ç±»å‹çš„äº’è¡¥æ€§
        if 'activity_type' in activity_data.columns:
            dev_activities = developers['activity_type'].value_counts(normalize=True)
            solver_activities = solvers['activity_type'].value_counts(normalize=True)
            
            # è®¡ç®—æ´»åŠ¨åˆ†å¸ƒçš„é‡å åº¦
            common_activities = set(dev_activities.index).intersection(set(solver_activities.index))
            
            if common_activities:
                overlap_score = 0
                for activity in common_activities:
                    overlap_score += min(dev_activities[activity], solver_activities[activity])
                
                complementarity_score = 1 - overlap_score  # äº’è¡¥æ€§ = 1 - é‡å åº¦
                result['evidence']['complementarity_score'] = complementarity_score
                result['evidence']['activity_overlap'] = overlap_score
                
                if complementarity_score > 0.6:
                    result['conclusion'] = f'Strong complementarity (score={complementarity_score:.3f})'
                elif complementarity_score > 0.3:
                    result['conclusion'] = f'Moderate complementarity (score={complementarity_score:.3f})'
                else:
                    result['conclusion'] = f'Weak complementarity (score={complementarity_score:.3f})'
            else:
                result['conclusion'] = 'No common activities found'
        else:
            result['conclusion'] = 'No activity type data'
        
        return result
    
    def _test_observer_contributor_transition(self, activity_data: pd.DataFrame) -> Dict[str, Any]:
        """æµ‹è¯•è§‚å¯Ÿè€…åˆ°è´¡çŒ®è€…çš„è½¬æ¢è·¯å¾„"""
        result = {'hypothesis': 'Observers transition to contributors through specific pathways', 'evidence': {}}
        
        # è¿™ä¸ªåˆ†æéœ€è¦æ—¶é—´åºåˆ—æ•°æ®æ¥è¿½è¸ªç”¨æˆ·çš„è§’è‰²å˜åŒ–
        # ç”±äºå½“å‰æ•°æ®ç»“æ„é™åˆ¶ï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªç®€åŒ–çš„åˆ†ææ¡†æ¶
        
        observers = activity_data[activity_data['role'] == 'observer']
        contributors = activity_data[activity_data['role'].isin(['casual_contributor', 'core_developer'])]
        
        if observers.empty or contributors.empty:
            result['conclusion'] = 'Insufficient data'
            return result
        
        # åˆ†æè§‚å¯Ÿè€…å’Œè´¡çŒ®è€…çš„æ´»åŠ¨æ¨¡å¼å·®å¼‚
        if 'activity_type' in activity_data.columns:
            observer_patterns = observers['activity_type'].value_counts(normalize=True)
            contributor_patterns = contributors['activity_type'].value_counts(normalize=True)
            
            # è¯†åˆ«è½¬æ¢è·¯å¾„ï¼ˆè§‚å¯Ÿè€…è¾ƒå¤šä½†è´¡çŒ®è€…ä¹Ÿæœ‰çš„æ´»åŠ¨ç±»å‹ï¼‰
            transition_activities = []
            for activity in observer_patterns.index:
                if activity in contributor_patterns.index:
                    # è½¬æ¢æŒ‡æ•° = observeræ¯”ä¾‹ * contributoræ¯”ä¾‹
                    transition_index = observer_patterns[activity] * contributor_patterns[activity]
                    transition_activities.append((activity, transition_index))
            
            # æ’åºæ‰¾åˆ°æœ€å¯èƒ½çš„è½¬æ¢è·¯å¾„
            transition_activities.sort(key=lambda x: x[1], reverse=True)
            
            result['evidence']['transition_pathways'] = transition_activities[:5]  # å‰5ä¸ªæœ€å¯èƒ½çš„è·¯å¾„
            
            if transition_activities:
                top_pathway = transition_activities[0]
                result['conclusion'] = f'Primary transition pathway: {top_pathway[0]} (index={top_pathway[1]:.3f})'
            else:
                result['conclusion'] = 'No clear transition pathways identified'
        else:
            result['conclusion'] = 'No activity type data for pathway analysis'
        
        return result
    
    def _calculate_dependency_summary(self, 
                                    collaboration_matrix: pd.DataFrame,
                                    temporal_dependencies: Dict[str, Any],
                                    complementarity_scores: Dict[str, float]) -> Dict[str, Any]:
        """è®¡ç®—ä¾èµ–å…³ç³»æ‘˜è¦ç»Ÿè®¡"""
        summary = {}
        
        # åä½œç½‘ç»œç»Ÿè®¡
        if not collaboration_matrix.empty:
            summary['collaboration_density'] = (collaboration_matrix > 0).sum().sum() / (len(collaboration_matrix) ** 2)
            summary['max_collaboration_strength'] = collaboration_matrix.max().max()
            summary['most_collaborative_role'] = collaboration_matrix.sum(axis=1).idxmax()
        
        # æ—¶é—´ä¾èµ–ç»Ÿè®¡
        if temporal_dependencies and 'granger_causality' in temporal_dependencies:
            granger_results = temporal_dependencies['granger_causality']
            significant_dependencies = 0
            
            for dep_key, lag_corrs in granger_results.items():
                for lag, corr_data in lag_corrs.items():
                    if isinstance(corr_data, dict) and corr_data.get('p_value', 1) < 0.05:
                        significant_dependencies += 1
            
            summary['significant_temporal_dependencies'] = significant_dependencies
            summary['total_temporal_tests'] = sum(len(lc) for lc in granger_results.values())
        
        # äº’è¡¥æ€§ç»Ÿè®¡
        if complementarity_scores:
            summary['avg_complementarity'] = np.mean(list(complementarity_scores.values()))
            summary['max_complementarity'] = max(complementarity_scores.values())
            summary['most_complementary_pair'] = max(complementarity_scores.keys(), 
                                                   key=lambda k: complementarity_scores[k])
        
        return summary
    
    def _save_symbiosis_results(self, results: Dict[str, Any]):
        """ä¿å­˜å…±ç”Ÿå…³ç³»åˆ†æç»“æœ"""
        # ä¿å­˜åä½œçŸ©é˜µ
        if 'collaboration_matrix' in results:
            collab_path = ANALYSIS_OUTPUT_DIR / "role_collaboration_matrix.csv"
            results['collaboration_matrix'].to_csv(collab_path, encoding='utf-8-sig')
            logger.info(f"åä½œçŸ©é˜µå·²ä¿å­˜è‡³: {collab_path}")
        
        # ä¿å­˜æ‘˜è¦ç»“æœ
        import json
        summary_path = ANALYSIS_OUTPUT_DIR / "role_symbiosis_analysis.json"
        
        # å¤„ç†ä¸èƒ½JSONåºåˆ—åŒ–çš„å¯¹è±¡
        json_results = {}
        for key, value in results.items():
            if key == 'collaboration_matrix':
                json_results[key] = value.to_dict()
            elif isinstance(value, dict):
                json_results[key] = value
            else:
                json_results[key] = str(value)
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(json_results, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"å…±ç”Ÿå…³ç³»åˆ†æç»“æœå·²ä¿å­˜è‡³: {summary_path}")
    
    def visualize_symbiosis_relationships(self):
        """å¯è§†åŒ–è§’è‰²å…±ç”Ÿå…³ç³»"""
        if not self.symbiosis_results:
            logger.warning("æ²¡æœ‰å…±ç”Ÿå…³ç³»åˆ†æç»“æœå¯è§†åŒ–")
            return
        
        plt.style.use(VISUALIZATION_CONFIG["style"])
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. åä½œçŸ©é˜µçƒ­åŠ›å›¾
        if 'collaboration_matrix' in self.symbiosis_results:
            collab_matrix = self.symbiosis_results['collaboration_matrix']
            if not collab_matrix.empty:
                sns.heatmap(collab_matrix, annot=True, cmap='YlOrRd', ax=axes[0,0], fmt='.3f')
                axes[0,0].set_title('Role Collaboration Matrix', fontsize=VISUALIZATION_CONFIG["title_font_size"])
                axes[0,0].set_xlabel('Target Role')
                axes[0,0].set_ylabel('Source Role')
        
        # 2. äº’è¡¥æ€§å¾—åˆ†
        if 'complementarity_scores' in self.symbiosis_results:
            comp_scores = self.symbiosis_results['complementarity_scores']
            if comp_scores:
                pairs = list(comp_scores.keys())
                scores = list(comp_scores.values())
                
                axes[0,1].bar(range(len(pairs)), scores)
                axes[0,1].set_title('Role Complementarity Scores', fontsize=VISUALIZATION_CONFIG["title_font_size"])
                axes[0,1].set_xlabel('Role Pairs')
                axes[0,1].set_ylabel('Complementarity Score')
                axes[0,1].set_xticks(range(len(pairs)))
                axes[0,1].set_xticklabels(pairs, rotation=45, ha='right')
        
        # 3. æ—¶é—´ä¾èµ–å…³ç³»ç½‘ç»œå›¾
        if 'temporal_dependencies' in self.symbiosis_results:
            self._plot_temporal_dependency_network(axes[1,0])
        
        # 4. å‡è®¾éªŒè¯ç»“æœ
        if 'symbiosis_hypotheses' in self.symbiosis_results:
            self._plot_hypothesis_validation_results(axes[1,1])
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        save_path = ANALYSIS_OUTPUT_DIR / "role_symbiosis_visualization.png"
        plt.savefig(save_path, dpi=VISUALIZATION_CONFIG["dpi"], bbox_inches='tight')
        logger.info(f"å…±ç”Ÿå…³ç³»å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")
        
        plt.show()
    
    def _plot_temporal_dependency_network(self, ax):
        """ç»˜åˆ¶æ—¶é—´ä¾èµ–å…³ç³»ç½‘ç»œå›¾"""
        ax.set_title('Temporal Dependency Network', fontsize=VISUALIZATION_CONFIG["title_font_size"])
        
        temporal_deps = self.symbiosis_results.get('temporal_dependencies', {})
        granger_results = temporal_deps.get('granger_causality', {})
        
        if not granger_results:
            ax.text(0.5, 0.5, 'No temporal dependency data', ha='center', va='center', transform=ax.transAxes)
            return
        
        # åˆ›å»ºç½‘ç»œå›¾
        G = nx.DiGraph()
        
        # æ·»åŠ æ˜¾è‘—çš„å› æœå…³ç³»ä½œä¸ºè¾¹
        for dep_key, lag_corrs in granger_results.items():
            if '_to_' in dep_key:
                source_role, target_role = dep_key.split('_to_')
                
                # æ‰¾åˆ°æœ€å¼ºçš„æ˜¾è‘—ç›¸å…³æ€§
                max_corr = 0
                best_lag = 0
                for lag, corr_data in lag_corrs.items():
                    if isinstance(corr_data, dict):
                        corr = corr_data.get('correlation', 0)
                        p_val = corr_data.get('p_value', 1)
                        if p_val < 0.05 and abs(corr) > abs(max_corr):
                            max_corr = corr
                            best_lag = lag
                
                if abs(max_corr) > 0.2:  # åªæ˜¾ç¤ºè¾ƒå¼ºçš„å…³ç³»
                    G.add_edge(source_role, target_role, weight=abs(max_corr), lag=best_lag)
        
        if G.number_of_nodes() > 0:
            pos = nx.spring_layout(G, k=1, iterations=50)
            
            # ç»˜åˆ¶èŠ‚ç‚¹
            nx.draw_networkx_nodes(G, pos, node_color='lightblue', 
                                 node_size=1000, alpha=0.7, ax=ax)
            
            # ç»˜åˆ¶è¾¹
            edges = G.edges()
            weights = [G[u][v]['weight'] for u, v in edges]
            nx.draw_networkx_edges(G, pos, width=[w*3 for w in weights], 
                                 alpha=0.6, edge_color='red', 
                                 arrowsize=20, arrowstyle='->', ax=ax)
            
            # ç»˜åˆ¶æ ‡ç­¾
            nx.draw_networkx_labels(G, pos, font_size=8, ax=ax)
        else:
            ax.text(0.5, 0.5, 'No significant temporal dependencies', 
                   ha='center', va='center', transform=ax.transAxes)
        
        ax.set_axis_off()
    
    def _plot_hypothesis_validation_results(self, ax):
        """ç»˜åˆ¶å‡è®¾éªŒè¯ç»“æœ"""
        ax.set_title('Symbiosis Hypothesis Validation', fontsize=VISUALIZATION_CONFIG["title_font_size"])
        
        hypotheses = self.symbiosis_results.get('symbiosis_hypotheses', {})
        
        if not hypotheses:
            ax.text(0.5, 0.5, 'No hypothesis validation data', ha='center', va='center', transform=ax.transAxes)
            return
        
        # æå–éªŒè¯ç»“æœ
        hypothesis_names = []
        evidence_scores = []
        
        for hyp_name, hyp_result in hypotheses.items():
            conclusion = hyp_result.get('conclusion', 'No evidence')
            
            # æ ¹æ®ç»“è®ºæ–‡æœ¬è¯„ä¼°è¯æ®å¼ºåº¦
            if 'Strong' in conclusion or 'strong' in conclusion:
                score = 3
            elif 'Moderate' in conclusion or 'moderate' in conclusion or 'Weak positive' in conclusion:
                score = 2
            elif 'Weak' in conclusion or 'weak' in conclusion:
                score = 1
            else:
                score = 0
            
            hypothesis_names.append(hyp_name.replace('_', ' ').title())
            evidence_scores.append(score)
        
        if hypothesis_names:
            colors = ['red' if s == 0 else 'orange' if s == 1 else 'yellow' if s == 2 else 'green' for s in evidence_scores]
            bars = ax.bar(range(len(hypothesis_names)), evidence_scores, color=colors, alpha=0.7)
            
            ax.set_xlabel('Hypotheses')
            ax.set_ylabel('Evidence Strength')
            ax.set_xticks(range(len(hypothesis_names)))
            ax.set_xticklabels(hypothesis_names, rotation=45, ha='right')
            ax.set_ylim(0, 3.5)
            
            # æ·»åŠ å›¾ä¾‹
            legend_elements = [
                plt.Rectangle((0,0),1,1, facecolor='green', alpha=0.7, label='Strong Evidence'),
                plt.Rectangle((0,0),1,1, facecolor='yellow', alpha=0.7, label='Moderate Evidence'),
                plt.Rectangle((0,0),1,1, facecolor='orange', alpha=0.7, label='Weak Evidence'),
                plt.Rectangle((0,0),1,1, facecolor='red', alpha=0.7, label='No Evidence')
            ]
            ax.legend(handles=legend_elements, loc='upper right', fontsize=8)
        else:
            ax.text(0.5, 0.5, 'No validation results', ha='center', va='center', transform=ax.transAxes)
    
    def generate_symbiosis_report(self) -> str:
        """ç”Ÿæˆè§’è‰²å…±ç”Ÿå…³ç³»åˆ†ææŠ¥å‘Š"""
        if not self.symbiosis_results:
            return "No symbiosis analysis results available."
        
        report_lines = [
            "=" * 60,
            "è§’è‰²å…±ç”Ÿå…³ç³»åˆ†ææŠ¥å‘Š",
            "Role Symbiosis Analysis Report",
            "=" * 60,
            ""
        ]
        
        # æ‘˜è¦ç»Ÿè®¡
        if 'summary_statistics' in self.symbiosis_results:
            summary = self.symbiosis_results['summary_statistics']
            report_lines.extend([
                "ğŸ“Š æ‘˜è¦ç»Ÿè®¡ (Summary Statistics):",
                f"  - åä½œç½‘ç»œå¯†åº¦: {summary.get('collaboration_density', 'N/A'):.3f}",
                f"  - æœ€å¤§åä½œå¼ºåº¦: {summary.get('max_collaboration_strength', 'N/A'):.3f}",
                f"  - æœ€åä½œçš„è§’è‰²: {summary.get('most_collaborative_role', 'N/A')}",
                f"  - æ˜¾è‘—æ—¶é—´ä¾èµ–å…³ç³»æ•°: {summary.get('significant_temporal_dependencies', 'N/A')}",
                f"  - å¹³å‡äº’è¡¥æ€§å¾—åˆ†: {summary.get('avg_complementarity', 'N/A'):.3f}",
                f"  - æœ€äº’è¡¥çš„è§’è‰²å¯¹: {summary.get('most_complementary_pair', 'N/A')}",
                ""
            ])
        
        # å‡è®¾éªŒè¯ç»“æœ
        if 'symbiosis_hypotheses' in self.symbiosis_results:
            report_lines.extend([
                "ğŸ”¬ å…±ç”Ÿå‡è®¾éªŒè¯ç»“æœ (Symbiosis Hypothesis Validation):",
                ""
            ])
            
            for hyp_name, hyp_result in self.symbiosis_results['symbiosis_hypotheses'].items():
                report_lines.extend([
                    f"å‡è®¾: {hyp_result.get('hypothesis', 'Unknown')}",
                    f"ç»“è®º: {hyp_result.get('conclusion', 'No conclusion')}",
                    ""
                ])
        
        # å…³é”®å‘ç°
        report_lines.extend([
            "ğŸ” å…³é”®å‘ç° (Key Findings):",
            ""
        ])
        
        # åŸºäºç»“æœç”Ÿæˆå…³é”®å‘ç°
        if 'collaboration_matrix' in self.symbiosis_results:
            collab_matrix = self.symbiosis_results['collaboration_matrix']
            if not collab_matrix.empty:
                max_collab = collab_matrix.max().max()
                max_pair = collab_matrix.stack().idxmax()
                report_lines.append(f"  - æœ€å¼ºåä½œå…³ç³»: {max_pair[0]} â†’ {max_pair[1]} (å¼ºåº¦: {max_collab:.3f})")
        
        if 'complementarity_scores' in self.symbiosis_results:
            comp_scores = self.symbiosis_results['complementarity_scores']
            if comp_scores:
                max_comp_pair = max(comp_scores.keys(), key=lambda k: comp_scores[k])
                max_comp_score = comp_scores[max_comp_pair]
                report_lines.append(f"  - æœ€å¼ºäº’è¡¥å…³ç³»: {max_comp_pair} (å¾—åˆ†: {max_comp_score:.3f})")
        
        report_lines.extend([
            "",
            "=" * 60,
            f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "=" * 60
        ])
        
        report = "\n".join(report_lines)
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = ANALYSIS_OUTPUT_DIR / "role_symbiosis_report.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"å…±ç”Ÿå…³ç³»åˆ†ææŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
        
        return report


def main():
    """ä¸»å‡½æ•°å…¥å£"""
    logger.info("è§’è‰²å…±ç”Ÿå…³ç³»åˆ†ææ¨¡å—æµ‹è¯•")
    
    # è¿™é‡Œåº”è¯¥åŠ è½½çœŸå®çš„ç”¨æˆ·è§’è‰²æ•°æ®å’Œæ´»åŠ¨æ•°æ®è¿›è¡Œæµ‹è¯•
    # ç”±äºæ²¡æœ‰çœŸå®æ•°æ®ï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªä½¿ç”¨ç¤ºä¾‹
    
    print("è§’è‰²å…±ç”Ÿå…³ç³»åˆ†æå™¨å·²å®ç°ä»¥ä¸‹åŠŸèƒ½:")
    print("1. è§’è‰²ä¾èµ–å…³ç³»åˆ†æ")
    print("2. æ—¶é—´åºåˆ—å› æœå…³ç³»æ£€éªŒ")
    print("3. è§’è‰²äº’è¡¥æ€§é‡åŒ–")
    print("4. çŸ¥è¯†æµåŠ¨æ¨¡å¼åˆ†æ")
    print("5. ç‰¹å®šå…±ç”Ÿå‡è®¾éªŒè¯")
    print("6. å¯è§†åŒ–å’ŒæŠ¥å‘Šç”Ÿæˆ")
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("analyzer = RoleSymbiosisAnalyzer(user_roles_df)")
    print("results = analyzer.analyze_role_dependencies(activity_data)")
    print("analyzer.visualize_symbiosis_relationships()")
    print("report = analyzer.generate_symbiosis_report()")


if __name__ == "__main__":
    main()
