#!/usr/bin/env python
"""
ä¸»åˆ†ææµæ°´çº¿è„šæœ¬
å±•ç¤ºå¦‚ä½•ä½¿ç”¨é‡æ„åçš„å¼€æºç”Ÿæ€ç³»ç»Ÿåˆ†ææ¡†æ¶
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

import pandas as pd
import logging
from datetime import datetime

# å¯¼å…¥é…ç½®
from config.settings import *
from config.api_config import api_config

# å¯¼å…¥åˆ†ææ¨¡å—
from src.network_analysis.dynamic_analysis import DynamicNetworkAnalyzer
from src.network_analysis.community_detection import CommunityDetector, DynamicCommunityAnalyzer
from src.user_analysis.role_clustering import MultiAlgorithmClustering
from src.user_analysis.role_evolution import RoleEvolutionAnalyzer
from src.causal_analysis.granger_causality import GrangerCausalityAnalyzer
from src.utils.logging_config import setup_logger

# è®¾ç½®æ—¥å¿—
logger = setup_logger(__name__, log_file=ANALYSIS_OUTPUT_DIR / "main_pipeline.log")


class AnalysisPipeline:
    """ä¸»åˆ†ææµæ°´çº¿"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†ææµæ°´çº¿"""
        self.results = {}
        logger.info("åˆå§‹åŒ–åˆ†ææµæ°´çº¿")
        
        # ç¡®ä¿æ‰€æœ‰è¾“å‡ºç›®å½•å­˜åœ¨
        ensure_directories()
    
    def run_network_analysis(self):
        """è¿è¡Œç½‘ç»œåˆ†æ"""
        logger.info("=" * 60)
        logger.info("å¼€å§‹ç½‘ç»œåˆ†æ...")
        logger.info("=" * 60)
        
        try:
            # 1. åŠ¨æ€ç½‘ç»œåˆ†æï¼ˆå¢å¼ºç‰ˆï¼ŒåŒ…å«å¤šç§ä¸­å¿ƒæ€§ï¼‰
            logger.info("1. è¿è¡ŒåŠ¨æ€ç½‘ç»œåˆ†æ...")
            dynamic_analyzer = DynamicNetworkAnalyzer()
            dynamic_results = dynamic_analyzer.run_complete_analysis(include_betweenness=True)
            self.results['dynamic_network'] = dynamic_results
            logger.info("âœ“ åŠ¨æ€ç½‘ç»œåˆ†æå®Œæˆ")
            
            # 2. ç¤¾åŒºæ£€æµ‹ç®—æ³•å¯¹æ¯”
            logger.info("2. è¿è¡Œç¤¾åŒºæ£€æµ‹ç®—æ³•å¯¹æ¯”...")
            
            # åŠ è½½ç½‘ç»œå›¾
            import networkx as nx
            graph_path = NETWORK_OUTPUT_DIR / FILENAMES["graph_file"]
            
            if graph_path.exists():
                G = nx.read_graphml(str(graph_path))
                
                # é™æ€ç¤¾åŒºæ£€æµ‹å¯¹æ¯”
                detector = CommunityDetector(G)
                comparison_df = detector.compare_algorithms()
                self.results['community_comparison'] = comparison_df
                
                # åŠ¨æ€ç¤¾åŒºåˆ†æ
                dynamic_community_analyzer = DynamicCommunityAnalyzer(
                    G, 
                    ANALYSIS_CONFIG["start_date"], 
                    ANALYSIS_CONFIG["end_date"],
                    algorithms=['louvain', 'leiden']  # æ ¹æ®å¯ç”¨ç®—æ³•è°ƒæ•´
                )
                dynamic_community_results = dynamic_community_analyzer.analyze_temporal_communities()
                self.results['dynamic_community'] = dynamic_community_results
                
                logger.info("âœ“ ç¤¾åŒºæ£€æµ‹åˆ†æå®Œæˆ")
            else:
                logger.warning(f"ç½‘ç»œå›¾æ–‡ä»¶ä¸å­˜åœ¨: {graph_path}")
                
        except Exception as e:
            logger.error(f"ç½‘ç»œåˆ†æå¤±è´¥: {e}")
    
    def run_user_analysis(self):
        """è¿è¡Œç”¨æˆ·åˆ†æ"""
        logger.info("=" * 60)
        logger.info("å¼€å§‹ç”¨æˆ·åˆ†æ...")
        logger.info("=" * 60)
        
        try:
            # 1. å¤šç®—æ³•è§’è‰²èšç±»
            logger.info("1. è¿è¡Œå¤šç®—æ³•è§’è‰²èšç±»...")
            
            features_path = ANALYSIS_OUTPUT_DIR / "user_behavior_features.csv"
            if features_path.exists():
                features_df = pd.read_csv(features_path)
                
                # å¤šç®—æ³•èšç±»å¯¹æ¯”
                clusterer = MultiAlgorithmClustering(features_df)
                optimal_k = clusterer.determine_optimal_k()
                recommended_k = optimal_k.get('silhouette', 6)
                
                comparison_df = clusterer.compare_all_algorithms(recommended_k)
                self.results['clustering_comparison'] = comparison_df
                
                # é€‰æ‹©æœ€ä½³ç®—æ³•
                best_algorithm = comparison_df.loc[comparison_df['silhouette_score'].idxmax(), 'algorithm'].lower()
                profile_df = clusterer.analyze_cluster_profiles(best_algorithm)
                result_df = clusterer.export_clustering_results(best_algorithm)
                
                self.results['user_clustering'] = {
                    'recommended_k': recommended_k,
                    'best_algorithm': best_algorithm,
                    'profile_df': profile_df,
                    'result_df': result_df
                }
                
                logger.info(f"âœ“ è§’è‰²èšç±»å®Œæˆï¼Œæœ€ä½³ç®—æ³•: {best_algorithm}, K={recommended_k}")
            else:
                logger.warning(f"ç”¨æˆ·ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {features_path}")
            
            # 2. è§’è‰²æ¼”åŒ–åˆ†æ
            logger.info("2. è¿è¡Œè§’è‰²æ¼”åŒ–åˆ†æ...")
            
            evolution_analyzer = RoleEvolutionAnalyzer(
                clustering_algorithm='kmeans',
                n_clusters=6
            )
            evolution_results = evolution_analyzer.run_complete_analysis()
            self.results['role_evolution'] = evolution_results
            
            logger.info("âœ“ è§’è‰²æ¼”åŒ–åˆ†æå®Œæˆ")
            
        except Exception as e:
            logger.error(f"ç”¨æˆ·åˆ†æå¤±è´¥: {e}")
    
    def run_causal_analysis(self):
        """è¿è¡Œå› æœåˆ†æ"""
        logger.info("=" * 60)
        logger.info("å¼€å§‹å› æœåˆ†æ...")
        logger.info("=" * 60)
        
        try:
            # 1. æ„å»ºé¢æ¿æ•°æ®
            logger.info("1. æ„å»ºé¢æ¿æ•°æ®...")
            from src.causal_analysis.panel_data_builder import PanelDataBuilder
            
            builder = PanelDataBuilder(frequency='M')
            ecosystem_panel = builder.build_ecosystem_panel()
            
            if not ecosystem_panel.empty:
                builder.save_panel_data(ecosystem_panel, "ecosystem_panel_data.csv", "ç”Ÿæ€ç³»ç»Ÿå±‚é¢")
                self.results['panel_data_construction'] = {
                    'n_entities': len(ecosystem_panel.index.get_level_values(0).unique()),
                    'n_time_periods': len(ecosystem_panel.index.get_level_values(1).unique()),
                    'n_observations': len(ecosystem_panel)
                }
                logger.info("âœ“ é¢æ¿æ•°æ®æ„å»ºå®Œæˆ")
            
            # 2. æ ¼å…°æ°å› æœæ£€éªŒ
            logger.info("2. è¿è¡Œæ ¼å…°æ°å› æœæ£€éªŒ...")
            
            panel_data_path = ANALYSIS_OUTPUT_DIR / "monthly_panel_data.csv"
            if panel_data_path.exists():
                panel_data = pd.read_csv(panel_data_path, index_col=0, parse_dates=True)
                
                granger_analyzer = GrangerCausalityAnalyzer(panel_data, max_lags=3)
                granger_results = granger_analyzer.run_complete_analysis()
                self.results['granger_causality'] = granger_results
                
                logger.info("âœ“ æ ¼å…°æ°å› æœæ£€éªŒå®Œæˆ")
                logger.info(f"å‘ç° {granger_results['summary']['significant_causal_relationships']} ä¸ªæ˜¾è‘—çš„å› æœå…³ç³»")
            else:
                logger.warning(f"é¢æ¿æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {panel_data_path}")
            
            # 3. å›ºå®šæ•ˆåº”æ¨¡å‹åˆ†æ
            logger.info("3. è¿è¡Œå›ºå®šæ•ˆåº”æ¨¡å‹åˆ†æ...")
            
            if not ecosystem_panel.empty:
                from src.causal_analysis.fixed_effects import PanelModelComparison
                
                # å®šä¹‰åˆ†æå˜é‡
                dependent_var = 'attract_stars_growth'
                independent_vars = ['mech_app_creation', 'mech_code_contrib', 'mech_problem_solving']
                
                # åˆ›å»ºæ¨¡å‹æ¯”è¾ƒåˆ†æå™¨
                comparator = PanelModelComparison(ecosystem_panel)
                
                # è¿è¡Œæ¨¡å‹æ¯”è¾ƒ
                comparison_results = comparator.compare_all_models(dependent_var, independent_vars)
                self.results['fixed_effects_models'] = comparison_results
                
                # å¯è§†åŒ–æ¯”è¾ƒç»“æœ
                comparator.visualize_model_comparison()
                
                logger.info("âœ“ å›ºå®šæ•ˆåº”æ¨¡å‹åˆ†æå®Œæˆ")
                logger.info(f"æ¯”è¾ƒäº† {len(comparison_results)} ä¸ªä¸åŒçš„é¢æ¿æ•°æ®æ¨¡å‹")
            
            # 4. å·¥å…·å˜é‡åˆ†æ
            logger.info("4. è¿è¡Œå·¥å…·å˜é‡åˆ†æ...")
            
            if not ecosystem_panel.empty:
                from src.causal_analysis.fixed_effects import InstrumentalVariablesAnalyzer
                
                iv_analyzer = InstrumentalVariablesAnalyzer(ecosystem_panel.reset_index())
                
                # è¯†åˆ«å·¥å…·å˜é‡
                iv_results = iv_analyzer.identify_instruments('mech_app_creation')
                
                if iv_results['valid_instruments']:
                    logger.info(f"æ‰¾åˆ° {len(iv_results['valid_instruments'])} ä¸ªæœ‰æ•ˆå·¥å…·å˜é‡")
                    
                    # ä½¿ç”¨å·¥å…·å˜é‡ä¼°è®¡
                    instruments = iv_results['valid_instruments'][:2]  # ä½¿ç”¨å‰2ä¸ªæœ€ä½³å·¥å…·å˜é‡
                    sls_results = iv_analyzer.estimate_2sls(
                        dependent_var, ['mech_app_creation'], instruments, 
                        ['mech_code_contrib', 'mech_problem_solving']
                    )
                    
                    if sls_results is not None:
                        self.results['instrumental_variables'] = {
                            'valid_instruments': iv_results['valid_instruments'],
                            'model_estimated': True,
                            'r_squared': sls_results.rsquared
                        }
                        logger.info("âœ“ å·¥å…·å˜é‡åˆ†æå®Œæˆ")
                else:
                    logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„å·¥å…·å˜é‡")
                    self.results['instrumental_variables'] = {
                        'valid_instruments': [],
                        'model_estimated': False
                    }
                
        except Exception as e:
            logger.error(f"å› æœåˆ†æå¤±è´¥: {e}")
    
    def run_advanced_models(self):
        """è¿è¡Œé«˜çº§æ¨¡å‹åˆ†æ"""
        logger.info("=" * 60)
        logger.info("å¼€å§‹é«˜çº§æ¨¡å‹åˆ†æ...")
        logger.info("=" * 60)
        
        try:
            # 1. HANæ¨¡å‹åˆ†æ
            logger.info("1. è¿è¡Œå¼‚æ„æ³¨æ„åŠ›ç½‘ç»œ(HAN)åˆ†æ...")
            
            from src.advanced_models.han_model import HANAnalyzer
            
            # åŠ è½½ç½‘ç»œå›¾
            graph_path = NETWORK_OUTPUT_DIR / FILENAMES["graph_file"]
            if graph_path.exists():
                import networkx as nx
                graph = nx.read_graphml(str(graph_path))
                
                # å¦‚æœå›¾å¤ªå¤§ï¼Œé‡‡æ ·è¿›è¡Œåˆ†æ
                if graph.number_of_nodes() > 500:
                    logger.info(f"å›¾è¾ƒå¤§({graph.number_of_nodes()}ä¸ªèŠ‚ç‚¹)ï¼Œé‡‡æ ·è¿›è¡ŒHANåˆ†æ...")
                    nodes_sample = list(graph.nodes())[:500]
                    graph = graph.subgraph(nodes_sample).copy()
                    logger.info(f"é‡‡æ ·åå›¾è§„æ¨¡: {graph.number_of_nodes()} ä¸ªèŠ‚ç‚¹")
                
                # åˆ›å»ºHANåˆ†æå™¨
                han_analyzer = HANAnalyzer(graph)
                
                # è¿è¡Œå®Œæ•´åˆ†æ
                han_results = han_analyzer.run_complete_analysis()
                self.results['han_analysis'] = han_results
                
                logger.info("âœ“ HANåˆ†æå®Œæˆ")
                logger.info(f"æ¨¡å‹æµ‹è¯•å‡†ç¡®ç‡: {han_results.get('test_results', {}).get('test_accuracy', 0):.4f}")
                
            else:
                logger.warning(f"æœªæ‰¾åˆ°ç½‘ç»œå›¾æ–‡ä»¶: {graph_path}")
            
            # 2. å›¾åµŒå…¥æ¯”è¾ƒåˆ†æ
            logger.info("2. è¿è¡Œå›¾åµŒå…¥æ–¹æ³•æ¯”è¾ƒ...")
            
            if graph_path.exists():
                from src.advanced_models.graph_embedding import ComparisonAnalyzer
                
                # åˆ›å»ºæ¯”è¾ƒåˆ†æå™¨
                comparator = ComparisonAnalyzer(graph)
                
                # è¿è¡Œæ¯”è¾ƒåˆ†æ
                embedding_results = comparator.compare_embedding_methods(embedding_dim=64)
                self.results['embedding_comparison'] = embedding_results
                
                logger.info("âœ“ å›¾åµŒå…¥æ¯”è¾ƒå®Œæˆ")
                logger.info(f"æ¯”è¾ƒäº† {len(embedding_results)} ç§åµŒå…¥æ–¹æ³•")
            
        except Exception as e:
            logger.error(f"é«˜çº§æ¨¡å‹åˆ†æå¤±è´¥: {e}")
    
    def generate_summary_report(self):
        """ç”Ÿæˆåˆ†ææ‘˜è¦æŠ¥å‘Š"""
        logger.info("=" * 60)
        logger.info("ç”Ÿæˆåˆ†ææ‘˜è¦æŠ¥å‘Š...")
        logger.info("=" * 60)
        
        try:
            report = {
                'analysis_timestamp': datetime.now().isoformat(),
                'analysis_period': {
                    'start_date': ANALYSIS_CONFIG["start_date"],
                    'end_date': ANALYSIS_CONFIG["end_date"]
                },
                'modules_executed': list(self.results.keys()),
                'summary': {}
            }
            
            # ç½‘ç»œåˆ†ææ‘˜è¦
            if 'dynamic_network' in self.results:
                dynamic_data = self.results['dynamic_network']
                if not dynamic_data.empty:
                    report['summary']['network_analysis'] = {
                        'total_time_points': len(dynamic_data),
                        'avg_nodes_per_month': dynamic_data['nodes_in_month'].mean(),
                        'avg_edges_per_month': dynamic_data['edges_in_month'].mean(),
                        'centrality_metrics_calculated': [col for col in dynamic_data.columns if 'centrality' in col]
                    }
            
            # ç¤¾åŒºæ£€æµ‹æ‘˜è¦
            if 'community_comparison' in self.results:
                community_data = self.results['community_comparison']
                if not community_data.empty:
                    best_community_algorithm = community_data.loc[community_data['modularity'].idxmax(), 'algorithm']
                    report['summary']['community_detection'] = {
                        'algorithms_compared': community_data['algorithm'].tolist(),
                        'best_algorithm_by_modularity': best_community_algorithm,
                        'best_modularity_score': community_data['modularity'].max()
                    }
            
            # ç”¨æˆ·åˆ†ææ‘˜è¦
            if 'user_clustering' in self.results:
                clustering_data = self.results['user_clustering']
                report['summary']['user_analysis'] = {
                    'recommended_clusters': clustering_data['recommended_k'],
                    'best_clustering_algorithm': clustering_data['best_algorithm'],
                    'total_users_analyzed': len(clustering_data['result_df']) if 'result_df' in clustering_data else 0
                }
            
            # è§’è‰²æ¼”åŒ–æ‘˜è¦
            if 'role_evolution' in self.results:
                evolution_data = self.results['role_evolution']
                report['summary']['role_evolution'] = {
                    'time_periods_analyzed': evolution_data.get('num_periods', 0),
                    'role_transitions_found': evolution_data.get('num_transitions', 0),
                    'clustering_algorithm_used': evolution_data.get('clustering_algorithm', 'unknown')
                }
            
            # å› æœåˆ†ææ‘˜è¦
            causal_summary = {}
            
            # é¢æ¿æ•°æ®æ„å»º
            if 'panel_data_construction' in self.results:
                panel_data = self.results['panel_data_construction']
                causal_summary['panel_data'] = {
                    'entities': panel_data['n_entities'],
                    'time_periods': panel_data['n_time_periods'],
                    'observations': panel_data['n_observations']
                }
            
            # æ ¼å…°æ°å› æœæ£€éªŒ
            if 'granger_causality' in self.results:
                granger_data = self.results['granger_causality']
                causal_summary['granger_causality'] = {
                    'variables_tested': granger_data['summary']['total_variables'],
                    'stationary_variables': granger_data['summary']['stationary_variables'],
                    'causal_relationships_found': granger_data['summary']['significant_causal_relationships'],
                    'total_tests_performed': granger_data['summary']['total_tests']
                }
            
            # å›ºå®šæ•ˆåº”æ¨¡å‹
            if 'fixed_effects_models' in self.results:
                fe_data = self.results['fixed_effects_models']
                causal_summary['fixed_effects'] = {
                    'models_compared': len([k for k in fe_data.keys() if k != 'hausman_test']),
                    'hausman_test_performed': 'hausman_test' in fe_data,
                    'recommended_model': fe_data.get('hausman_test', {}).get('recommendation', 'unknown')
                }
            
            # å·¥å…·å˜é‡åˆ†æ
            if 'instrumental_variables' in self.results:
                iv_data = self.results['instrumental_variables']
                causal_summary['instrumental_variables'] = {
                    'valid_instruments_found': len(iv_data['valid_instruments']),
                    'model_estimated': iv_data['model_estimated'],
                    'r_squared': iv_data.get('r_squared', None)
                }
            
            if causal_summary:
                report['summary']['causal_analysis'] = causal_summary
            
            # é«˜çº§æ¨¡å‹åˆ†ææ‘˜è¦
            advanced_summary = {}
            
            # HANæ¨¡å‹åˆ†æ
            if 'han_analysis' in self.results:
                han_data = self.results['han_analysis']
                advanced_summary['han_model'] = {
                    'model_parameters': han_data.get('model_info', {}).get('total_parameters', 0),
                    'test_accuracy': han_data.get('test_results', {}).get('test_accuracy', 0),
                    'node_types': han_data.get('hetero_data_info', {}).get('node_types', 0),
                    'edge_types': han_data.get('hetero_data_info', {}).get('edge_types', 0),
                    'training_epochs': han_data.get('training_results', {}).get('epochs_trained', 0)
                }
            
            # å›¾åµŒå…¥æ¯”è¾ƒ
            if 'embedding_comparison' in self.results:
                embedding_data = self.results['embedding_comparison']
                advanced_summary['graph_embedding'] = {
                    'methods_compared': len(embedding_data),
                    'embedding_methods': list(embedding_data.keys())
                }
            
            if advanced_summary:
                report['summary']['advanced_models'] = advanced_summary
            
            # ä¿å­˜æŠ¥å‘Š
            import json
            report_path = ANALYSIS_OUTPUT_DIR / "analysis_summary_report.json"
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"âœ“ åˆ†ææ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
            
            # æ‰“å°æ‘˜è¦åˆ°æ§åˆ¶å°
            self._print_summary(report)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ‘˜è¦æŠ¥å‘Šå¤±è´¥: {e}")
    
    def _print_summary(self, report):
        """æ‰“å°åˆ†ææ‘˜è¦åˆ°æ§åˆ¶å°"""
        print("\n" + "=" * 80)
        print("                    å¼€æºç”Ÿæ€ç³»ç»Ÿåˆ†æ - æœ€ç»ˆæŠ¥å‘Š")
        print("=" * 80)
        
        print(f"åˆ†ææ—¶é—´: {report['analysis_timestamp']}")
        print(f"åˆ†ææœŸé—´: {report['analysis_period']['start_date']} åˆ° {report['analysis_period']['end_date']}")
        print(f"æ‰§è¡Œæ¨¡å—: {', '.join(report['modules_executed'])}")
        
        summary = report.get('summary', {})
        
        if 'network_analysis' in summary:
            net_sum = summary['network_analysis']
            print(f"\nğŸ“Š ç½‘ç»œåˆ†æ:")
            print(f"  - åˆ†ææ—¶é—´ç‚¹æ•°: {net_sum['total_time_points']}")
            print(f"  - å¹³å‡æœˆåº¦èŠ‚ç‚¹æ•°: {net_sum['avg_nodes_per_month']:.0f}")
            print(f"  - å¹³å‡æœˆåº¦è¾¹æ•°: {net_sum['avg_edges_per_month']:.0f}")
            print(f"  - è®¡ç®—çš„ä¸­å¿ƒæ€§æŒ‡æ ‡: {len(net_sum['centrality_metrics_calculated'])}")
        
        if 'community_detection' in summary:
            comm_sum = summary['community_detection']
            print(f"\nğŸ˜ï¸  ç¤¾åŒºæ£€æµ‹:")
            print(f"  - å¯¹æ¯”ç®—æ³•æ•°: {len(comm_sum['algorithms_compared'])}")
            print(f"  - æœ€ä½³ç®—æ³•: {comm_sum['best_algorithm_by_modularity']}")
            print(f"  - æœ€é«˜æ¨¡å—åº¦: {comm_sum['best_modularity_score']:.3f}")
        
        if 'user_analysis' in summary:
            user_sum = summary['user_analysis']
            print(f"\nğŸ‘¥ ç”¨æˆ·åˆ†æ:")
            print(f"  - æ¨èèšç±»æ•°: {user_sum['recommended_clusters']}")
            print(f"  - æœ€ä½³èšç±»ç®—æ³•: {user_sum['best_clustering_algorithm']}")
            print(f"  - åˆ†æç”¨æˆ·æ•°: {user_sum['total_users_analyzed']}")
        
        if 'role_evolution' in summary:
            evol_sum = summary['role_evolution']
            print(f"\nğŸ”„ è§’è‰²æ¼”åŒ–:")
            print(f"  - åˆ†ææ—¶é—´æ®µæ•°: {evol_sum['time_periods_analyzed']}")
            print(f"  - å‘ç°è§’è‰²è½¬ç§»æ•°: {evol_sum['role_transitions_found']}")
            print(f"  - ä½¿ç”¨ç®—æ³•: {evol_sum['clustering_algorithm_used']}")
        
        if 'causal_analysis' in summary:
            causal_sum = summary['causal_analysis']
            print(f"\nğŸ”— å› æœåˆ†æ:")
            
            # é¢æ¿æ•°æ®
            if 'panel_data' in causal_sum:
                panel_data = causal_sum['panel_data']
                print(f"  ğŸ“Š é¢æ¿æ•°æ®: {panel_data['entities']} ä¸ªå®ä½“, {panel_data['time_periods']} ä¸ªæ—¶é—´æ®µ, {panel_data['observations']} ä¸ªè§‚æµ‹å€¼")
            
            # æ ¼å…°æ°å› æœæ£€éªŒ
            if 'granger_causality' in causal_sum:
                granger_data = causal_sum['granger_causality']
                print(f"  â° æ ¼å…°æ°å› æœ: æµ‹è¯• {granger_data['variables_tested']} ä¸ªå˜é‡, å‘ç° {granger_data['causal_relationships_found']} ä¸ªå› æœå…³ç³»")
            
            # å›ºå®šæ•ˆåº”æ¨¡å‹
            if 'fixed_effects' in causal_sum:
                fe_data = causal_sum['fixed_effects']
                print(f"  ğŸ›ï¸  å›ºå®šæ•ˆåº”: æ¯”è¾ƒ {fe_data['models_compared']} ä¸ªæ¨¡å‹, æ¨è {fe_data['recommended_model']}")
            
            # å·¥å…·å˜é‡åˆ†æ
            if 'instrumental_variables' in causal_sum:
                iv_data = causal_sum['instrumental_variables']
                print(f"  ğŸ”§ å·¥å…·å˜é‡: æ‰¾åˆ° {iv_data['valid_instruments_found']} ä¸ªæœ‰æ•ˆå·¥å…·å˜é‡, æ¨¡å‹ä¼°è®¡{'æˆåŠŸ' if iv_data['model_estimated'] else 'å¤±è´¥'}")
        
        if 'advanced_models' in summary:
            advanced_sum = summary['advanced_models']
            print(f"\nğŸ§  é«˜çº§æ¨¡å‹:")
            
            # HANæ¨¡å‹
            if 'han_model' in advanced_sum:
                han_data = advanced_sum['han_model']
                print(f"  ğŸ¯ HANæ¨¡å‹: {han_data['model_parameters']:,} å‚æ•°, æµ‹è¯•å‡†ç¡®ç‡ {han_data['test_accuracy']:.4f}")
                print(f"    - å¼‚æ„å›¾: {han_data['node_types']} ç§èŠ‚ç‚¹ç±»å‹, {han_data['edge_types']} ç§è¾¹ç±»å‹")
                print(f"    - è®­ç»ƒè½®æ•°: {han_data['training_epochs']}")
            
            # å›¾åµŒå…¥æ¯”è¾ƒ
            if 'graph_embedding' in advanced_sum:
                embedding_data = advanced_sum['graph_embedding']
                print(f"  ğŸ“Š å›¾åµŒå…¥: æ¯”è¾ƒ {embedding_data['methods_compared']} ç§æ–¹æ³• ({', '.join(embedding_data['embedding_methods'])})")
        
        print("\n" + "=" * 80)
        print("åˆ†æå®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° results/analysis_output/ ç›®å½•")
        print("=" * 80)
    
    def run_complete_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„åˆ†ææµæ°´çº¿"""
        start_time = datetime.now()
        logger.info("ğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´çš„å¼€æºç”Ÿæ€ç³»ç»Ÿåˆ†ææµæ°´çº¿...")
        
        try:
            # 1. ç½‘ç»œåˆ†æ
            self.run_network_analysis()
            
            # 2. ç”¨æˆ·åˆ†æ
            self.run_user_analysis()
            
            # 3. å› æœåˆ†æ
            self.run_causal_analysis()
            
            # 4. é«˜çº§æ¨¡å‹åˆ†æ
            self.run_advanced_models()
            
            # 5. ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
            self.generate_summary_report()
            
            end_time = datetime.now()
            duration = end_time - start_time
            
            logger.info(f"ğŸ‰ åˆ†ææµæ°´çº¿æ‰§è¡Œå®Œæˆï¼æ€»è€—æ—¶: {duration}")
            
        except Exception as e:
            logger.error(f"åˆ†ææµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
            raise


def main():
    """ä¸»å‡½æ•°å…¥å£"""
    print("å¼€æºç”Ÿæ€ç³»ç»Ÿç»„ç»‡ç»“æ„ä¸åˆ›æ–°æ¨¡å¼åˆ†æ")
    print("=" * 50)
    
    # æ£€æŸ¥APIé…ç½®
    if not api_config.validate_github_token():
        print("âš ï¸  è­¦å‘Š: GitHub Tokené…ç½®æ— æ•ˆ")
    
    if not api_config.validate_openai_key():
        print("âš ï¸  è­¦å‘Š: OpenAI API Keyé…ç½®æ— æ•ˆ")
    
    # åˆ›å»ºå¹¶è¿è¡Œåˆ†ææµæ°´çº¿
    pipeline = AnalysisPipeline()
    
    try:
        pipeline.run_complete_pipeline()
        print("\nâœ… åˆ†ææµæ°´çº¿æˆåŠŸæ‰§è¡Œå®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  åˆ†æè¢«ç”¨æˆ·ä¸­æ–­")
        logger.info("åˆ†æè¢«ç”¨æˆ·ä¸­æ–­")
        
    except Exception as e:
        print(f"\nâŒ åˆ†ææµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
        logger.error(f"åˆ†ææµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
