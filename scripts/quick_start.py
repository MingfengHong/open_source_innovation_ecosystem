#!/usr/bin/env python
"""
å¿«é€Ÿå¯åŠ¨è„šæœ¬
æä¾›ç®€åŒ–çš„åˆ†ææµç¨‹ï¼Œç”¨äºå¿«é€ŸéªŒè¯é¡¹ç›®åŠŸèƒ½
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

import pandas as pd
import numpy as np
import networkx as nx
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

from src.utils.logging_config import setup_logger

# è®¾ç½®æ—¥å¿—
logger = setup_logger(__name__)

def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹æ•°æ®ç”¨äºå¿«é€ŸéªŒè¯"""
    print("ğŸ”§ åˆ›å»ºç¤ºä¾‹æ•°æ®...")
    
    from config.settings import FINAL_ANALYSIS_DATA_DIR, CLASSIFICATION_OUTPUT_DIR
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    FINAL_ANALYSIS_DATA_DIR.mkdir(parents=True, exist_ok=True)
    CLASSIFICATION_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºç¤ºä¾‹ä»“åº“æ•°æ®
    repos_data = []
    for i in range(50):
        repos_data.append({
            'repo_id': i,
            'repo_name': f'test_repo_{i}',
            'owner': f'owner_{i%10}',
            'stars': np.random.randint(0, 1000),
            'forks': np.random.randint(0, 100),
            'created_at': '2022-01-01T00:00:00Z'
        })
    
    repos_df = pd.DataFrame(repos_data)
    repos_df.to_csv(FINAL_ANALYSIS_DATA_DIR / 'repos.csv', index=False)
    
    # åˆ›å»ºç¤ºä¾‹ç”¨æˆ·æ•°æ®
    users_data = []
    for i in range(100):
        users_data.append({
            'user_id': i,
            'login': f'user_{i}',
            'user_type': 'User'
        })
    
    users_df = pd.DataFrame(users_data)
    users_df.to_csv(FINAL_ANALYSIS_DATA_DIR / 'users.csv', index=False)
    
    # åˆ›å»ºç¤ºä¾‹PRæ•°æ®
    prs_data = []
    for i in range(200):
        prs_data.append({
            'pr_id': i,
            'repo_id': np.random.randint(0, 50),
            'author_id': np.random.randint(0, 100),
            'created_at': f'2023-{np.random.randint(1,13):02d}-01T00:00:00Z',
            'state': np.random.choice(['open', 'closed', 'merged']),
            'additions': np.random.randint(1, 500),
            'deletions': np.random.randint(1, 200)
        })
    
    prs_df = pd.DataFrame(prs_data)
    prs_df.to_csv(FINAL_ANALYSIS_DATA_DIR / 'prs.csv', index=False)
    
    # åˆ›å»ºç¤ºä¾‹Issueæ•°æ®
    issues_data = []
    for i in range(150):
        issues_data.append({
            'issue_id': i,
            'repo_id': np.random.randint(0, 50),
            'author_id': np.random.randint(0, 100),
            'created_at': f'2023-{np.random.randint(1,13):02d}-01T00:00:00Z',
            'state': np.random.choice(['open', 'closed'])
        })
    
    issues_df = pd.DataFrame(issues_data)
    issues_df.to_csv(FINAL_ANALYSIS_DATA_DIR / 'issues.csv', index=False)
    
    # åˆ›å»ºç¤ºä¾‹Staræ•°æ®
    stars_data = []
    for i in range(300):
        stars_data.append({
            'user_id': np.random.randint(0, 100),
            'repo_id': np.random.randint(0, 50),
            'starred_at': f'2023-{np.random.randint(1,13):02d}-01T00:00:00Z'
        })
    
    stars_df = pd.DataFrame(stars_data)
    stars_df.to_csv(FINAL_ANALYSIS_DATA_DIR / 'stars.csv', index=False)
    
    # åˆ›å»ºç¤ºä¾‹åˆ†ç±»æ•°æ®
    classification_data = []
    for i in range(50):
        classification_data.append({
            'repo_name': f'test_repo_{i}',
            'primary_role': np.random.choice(['application', 'library', 'tool', 'tutorial', 'integration']),
            'is_monorepo': np.random.choice([True, False]),
            'has_documentation': np.random.choice([True, False])
        })
    
    classification_df = pd.DataFrame(classification_data)
    classification_df.to_csv(CLASSIFICATION_OUTPUT_DIR / 'repos_classified.csv', index=False)
    
    print("âœ… ç¤ºä¾‹æ•°æ®åˆ›å»ºå®Œæˆ")
    return True

def run_quick_network_analysis():
    """è¿è¡Œå¿«é€Ÿç½‘ç»œåˆ†æ"""
    print("\nğŸ“Š è¿è¡Œç½‘ç»œåˆ†æ...")
    
    try:
        from src.network_analysis.dynamic_analysis import DynamicNetworkAnalyzer
        
        # åˆ›å»ºåˆ†æå™¨
        analyzer = DynamicNetworkAnalyzer()
        
        # è¿è¡Œç®€åŒ–åˆ†æ
        print("   - åŠ è½½æ•°æ®...")
        results = analyzer.run_complete_analysis(
            include_betweenness=True,
            include_eigenvector=True,
            max_months=6  # é™åˆ¶åˆ†ææœˆæ•°ä»¥èŠ‚çœæ—¶é—´
        )
        
        if results:
            print("   âœ… ç½‘ç»œåˆ†æå®Œæˆ")
            print(f"   - åˆ†æäº† {results.get('months_analyzed', 0)} ä¸ªæœˆçš„æ•°æ®")
            return True
        else:
            print("   âš ï¸  ç½‘ç»œåˆ†ææœªè¿”å›ç»“æœ")
            return False
            
    except Exception as e:
        print(f"   âŒ ç½‘ç»œåˆ†æå¤±è´¥: {e}")
        return False

def run_quick_user_analysis():
    """è¿è¡Œå¿«é€Ÿç”¨æˆ·åˆ†æ"""
    print("\nğŸ‘¥ è¿è¡Œç”¨æˆ·åˆ†æ...")
    
    try:
        from src.user_analysis.role_clustering import MultiAlgorithmClustering
        
        # åˆ›å»ºç¤ºä¾‹ç”¨æˆ·ç‰¹å¾æ•°æ®
        from config.settings import ANALYSIS_OUTPUT_DIR
        ANALYSIS_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆç¤ºä¾‹ç‰¹å¾
        n_users = 100
        features_data = []
        
        for i in range(n_users):
            features_data.append({
                'user_id': i,
                'login': f'user_{i}',
                'pr_count': np.random.randint(0, 50),
                'issue_count': np.random.randint(0, 30),
                'star_count': np.random.randint(0, 100),
                'total_additions': np.random.randint(0, 5000),
                'total_deletions': np.random.randint(0, 2000),
                'repo_count': np.random.randint(1, 10),
                'code_focus_ratio': np.random.random(),
                'interaction_diversity': np.random.randint(1, 5)
            })
        
        features_df = pd.DataFrame(features_data)
        features_path = ANALYSIS_OUTPUT_DIR / 'user_behavior_features.csv'
        features_df.to_csv(features_path, index=False)
        
        # è¿è¡Œèšç±»åˆ†æ
        print("   - æ‰§è¡Œå¤šç®—æ³•èšç±»...")
        clusterer = MultiAlgorithmClustering(features_df)
        clustering_results = clusterer.compare_all_algorithms(n_clusters=5)
        
        if clustering_results:
            print("   âœ… ç”¨æˆ·åˆ†æå®Œæˆ")
            print(f"   - æ¯”è¾ƒäº† {len(clustering_results)} ç§èšç±»ç®—æ³•")
            return True
        else:
            print("   âš ï¸  ç”¨æˆ·åˆ†ææœªè¿”å›ç»“æœ")
            return False
            
    except Exception as e:
        print(f"   âŒ ç”¨æˆ·åˆ†æå¤±è´¥: {e}")
        return False

def run_quick_causal_analysis():
    """è¿è¡Œå¿«é€Ÿå› æœåˆ†æ"""
    print("\nğŸ”— è¿è¡Œå› æœåˆ†æ...")
    
    try:
        # åˆ›å»ºç¤ºä¾‹æœˆåº¦é¢æ¿æ•°æ®
        from config.settings import ANALYSIS_OUTPUT_DIR
        
        # ç”Ÿæˆç¤ºä¾‹æœˆåº¦æ•°æ®
        dates = pd.date_range('2023-01-01', '2023-12-01', freq='MS')
        panel_data = []
        
        for date in dates:
            panel_data.append({
                'attract_stars_growth': np.random.random() * 100,
                'mech_app_creation': np.random.random() * 50,
                'mech_code_contrib': np.random.random() * 200,
                'mech_problem_solving': np.random.random() * 80,
                'robust_social_capital': np.random.random() * 150,
                'robust_cognitive_capital': np.random.random() * 120,
                'innovate_novel_solutions': np.random.random() * 90,
                'innovate_paradigm_shift': np.random.random() * 60
            })
        
        panel_df = pd.DataFrame(panel_data, index=dates)
        panel_path = ANALYSIS_OUTPUT_DIR / 'monthly_panel_data.csv'
        panel_df.to_csv(panel_path)
        
        # è¿è¡Œæ ¼å…°æ°å› æœæ£€éªŒ
        print("   - æ‰§è¡Œæ ¼å…°æ°å› æœæ£€éªŒ...")
        from src.causal_analysis.granger_causality import GrangerCausalityAnalyzer
        
        granger_analyzer = GrangerCausalityAnalyzer(panel_df, max_lags=2)
        granger_results = granger_analyzer.run_complete_analysis()
        
        if granger_results:
            print("   âœ… å› æœåˆ†æå®Œæˆ")
            print(f"   - å‘ç° {granger_results['summary']['significant_causal_relationships']} ä¸ªå› æœå…³ç³»")
            return True
        else:
            print("   âš ï¸  å› æœåˆ†ææœªè¿”å›ç»“æœ")
            return False
            
    except Exception as e:
        print(f"   âŒ å› æœåˆ†æå¤±è´¥: {e}")
        return False

def run_quick_han_demo():
    """è¿è¡ŒHANæ¨¡å‹å¿«é€Ÿæ¼”ç¤º"""
    print("\nğŸ§  è¿è¡ŒHANæ¨¡å‹æ¼”ç¤º...")
    
    try:
        from src.advanced_models.han_model import HANAnalyzer
        import networkx as nx
        
        # åˆ›å»ºç®€å•çš„å¼‚æ„å›¾
        print("   - åˆ›å»ºç¤ºä¾‹å¼‚æ„å›¾...")
        G = nx.Graph()
        
        # æ·»åŠ ä¸åŒç±»å‹çš„èŠ‚ç‚¹
        for i in range(20):
            G.add_node(f"user_{i}", type='user')
        for i in range(10):
            G.add_node(f"repo_{i}", type='repo', primary_role='application')
        for i in range(15):
            G.add_node(f"pr_{i}", type='pr')
        
        # æ·»åŠ è¾¹
        for i in range(20):
            # ç”¨æˆ·-ä»“åº“è¾¹
            G.add_edge(f"user_{i}", f"repo_{i%10}", type='star')
            # ç”¨æˆ·-PRè¾¹
            if i < 15:
                G.add_edge(f"user_{i}", f"pr_{i}", type='create')
                G.add_edge(f"pr_{i}", f"repo_{i%10}", type='belongs_to')
        
        print("   - åˆå§‹åŒ–HANåˆ†æå™¨...")
        analyzer = HANAnalyzer(G)
        
        print("   - è¿è¡ŒHANåˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰...")
        # è¿è¡Œç®€åŒ–çš„åˆ†æ
        hetero_data = analyzer.prepare_data()
        model = analyzer.create_model(hidden_dim=32, num_heads=2, num_layers=1)
        
        # å¿«é€Ÿè®­ç»ƒ
        training_history = analyzer.train_model(epochs=10, learning_rate=0.01)
        
        if training_history:
            print("   âœ… HANæ¼”ç¤ºå®Œæˆ")
            print(f"   - è®­ç»ƒäº† {len(training_history['train_loss'])} è½®")
            return True
        else:
            print("   âš ï¸  HANæ¼”ç¤ºæœªè¿”å›ç»“æœ")
            return False
            
    except Exception as e:
        print(f"   âš ï¸  HANæ¼”ç¤ºè·³è¿‡ï¼ˆéœ€è¦PyTorch Geometricï¼‰: {e}")
        return True  # ä¸ç®—ä½œå¤±è´¥ï¼Œå› ä¸ºè¿™æ˜¯å¯é€‰åŠŸèƒ½


def run_quick_subcenter_demo():
    """è¿è¡Œå­ä¸­å¿ƒè¯†åˆ«å¿«é€Ÿæ¼”ç¤º"""
    print("\nğŸ¯ è¿è¡Œå­ä¸­å¿ƒè¯†åˆ«æ¼”ç¤º...")
    
    try:
        from src.network_analysis.community_detection import CommunityDetector, SubCenterDetector
        import networkx as nx
        
        # åˆ›å»ºç®€å•æµ‹è¯•ç½‘ç»œ
        print("   - åˆ›å»ºæµ‹è¯•ç½‘ç»œ...")
        G = nx.Graph()
        
        # æ·»åŠ æ ¸å¿ƒå›¢é˜Ÿ
        G.add_node("core_1", type="user", login="hwchase17", user_type="core")
        G.add_node("core_2", type="user", login="agola11", user_type="core")
        
        # æ·»åŠ ç¤¾åŒºç”¨æˆ·ï¼ˆæ½œåœ¨å­ä¸­å¿ƒï¼‰
        community_users = []
        for i in range(3, 15):
            user_id = f"user_{i}"
            G.add_node(user_id, type="user", login=f"community_user_{i}", user_type="community")
            community_users.append(user_id)
        
        # æ·»åŠ ä»“åº“
        for i in range(20):
            repo_type = "application" if i < 10 else "library"
            G.add_node(f"repo_{i}", type="repo", name=f"repo_{i}", 
                      primary_role=repo_type, stars=100)
        
        # æ·»åŠ PR
        for i in range(30):
            contrib_type = "code" if i < 20 else "doc"
            G.add_node(f"pr_{i}", type="pr", contribution_type=contrib_type)
        
        # åˆ›å»ºè¿æ¥æ¨¡å¼ï¼ˆæ¨¡æ‹Ÿå­ä¸­å¿ƒï¼‰
        # æ ¸å¿ƒå›¢é˜Ÿè¿æ¥ä¸»è¦ä»“åº“
        for core in ["core_1", "core_2"]:
            for i in range(5):
                G.add_edge(core, f"repo_{i}", type="maintain")
        
        # åˆ›å»ºåº”ç”¨å¼€å‘å­ä¸­å¿ƒ
        app_devs = community_users[:6]
        for user in app_devs:
            # è¿æ¥åº”ç”¨ä»“åº“
            for i in range(5, 10):
                G.add_edge(user, f"repo_{i}", type="contribute")
            # è¿æ¥ä»£ç PR
            for i in range(3):
                G.add_edge(user, f"pr_{i*2}", type="create", contribution_type="code")
        
        # åˆ›å»ºæ–‡æ¡£è´¡çŒ®å­ä¸­å¿ƒ
        doc_contribs = community_users[6:]
        for user in doc_contribs:
            # è¿æ¥å„ç§ä»“åº“
            for i in range(10, 15):
                G.add_edge(user, f"repo_{i}", type="document")
            # è¿æ¥æ–‡æ¡£PR
            for i in range(20, 25):
                G.add_edge(user, f"pr_{i}", type="create", contribution_type="doc")
        
        print(f"   - æµ‹è¯•ç½‘ç»œ: {G.number_of_nodes()} èŠ‚ç‚¹, {G.number_of_edges()} è¾¹")
        
        # ç¤¾åŒºæ£€æµ‹
        print("   - æ‰§è¡Œç¤¾åŒºæ£€æµ‹...")
        detector = CommunityDetector(G)
        communities = detector.detect_louvain_communities()
        print(f"   - æ£€æµ‹åˆ° {len(communities)} ä¸ªç¤¾åŒº")
        
        # å­ä¸­å¿ƒè¯†åˆ«
        print("   - è¯†åˆ«å­ä¸­å¿ƒ...")
        subcenter_detector = SubCenterDetector(
            G, 
            core_team_logins=["hwchase17", "agola11"],
            min_subcenter_size=3,
            innovation_threshold=0.01
        )
        
        subcenters = subcenter_detector.identify_sub_centers(communities, 'louvain')
        
        if subcenters:
            print(f"   âœ… è¯†åˆ«å‡º {len(subcenters)} ä¸ªå­ä¸­å¿ƒ")
            for i, sc in enumerate(subcenters):
                print(f"     å­ä¸­å¿ƒ{i+1}: è§„æ¨¡{sc['size']}, åŠŸèƒ½{sc['functional_type']}, åˆ›æ–°{sc['innovation_score']:.3f}")
            return True
        else:
            print("   âš ï¸  æœªè¯†åˆ«å‡ºå­ä¸­å¿ƒ")
            return False
            
    except Exception as e:
        print(f"   âŒ å­ä¸­å¿ƒè¯†åˆ«æ¼”ç¤ºå¤±è´¥: {e}")
        return False

def generate_quick_report():
    """ç”Ÿæˆå¿«é€ŸæŠ¥å‘Š"""
    print("\nğŸ“„ ç”Ÿæˆå¿«é€Ÿåˆ†ææŠ¥å‘Š...")
    
    from config.settings import ANALYSIS_OUTPUT_DIR
    
    report = {
        'timestamp': datetime.now().isoformat(),
        'mode': 'quick_start_demo',
        'status': 'completed',
        'components_tested': [
            'network_analysis',
            'user_analysis', 
            'causal_analysis',
            'han_model_demo'
        ],
        'note': 'This is a quick start demo with synthetic data'
    }
    
    import json
    report_path = ANALYSIS_OUTPUT_DIR / 'quick_start_report.json'
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"   âœ… æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€æºç”Ÿæ€ç³»ç»Ÿåˆ†æ - å¿«é€Ÿå¯åŠ¨æ¼”ç¤º")
    print("=" * 60)
    print("è¿™æ˜¯ä¸€ä¸ªå¿«é€ŸéªŒè¯é¡¹ç›®åŠŸèƒ½çš„æ¼”ç¤ºè„šæœ¬")
    print("ä½¿ç”¨åˆæˆæ•°æ®æ¥éªŒè¯å„ä¸ªåˆ†ææ¨¡å—æ˜¯å¦æ­£å¸¸å·¥ä½œ")
    print("=" * 60)
    
    start_time = datetime.now()
    
    # æ‰§è¡Œå¿«é€Ÿæµ‹è¯•
    tests = [
        ("åˆ›å»ºç¤ºä¾‹æ•°æ®", create_sample_data),
        ("ç½‘ç»œåˆ†æ", run_quick_network_analysis), 
        ("ç”¨æˆ·åˆ†æ", run_quick_user_analysis),
        ("å› æœåˆ†æ", run_quick_causal_analysis),
        ("å­ä¸­å¿ƒè¯†åˆ«æ¼”ç¤º", run_quick_subcenter_demo),
        ("HANæ¨¡å‹æ¼”ç¤º", run_quick_han_demo),
        ("ç”ŸæˆæŠ¥å‘Š", generate_quick_report)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            result = test_func()
            results.append((test_name, result))
            
            if result:
                print(f"âœ… {test_name} - æˆåŠŸ")
            else:
                print(f"âš ï¸  {test_name} - éƒ¨åˆ†æˆåŠŸæˆ–è·³è¿‡")
                
        except Exception as e:
            print(f"âŒ {test_name} - å¤±è´¥: {e}")
            results.append((test_name, False))
    
    # æ±‡æ€»ç»“æœ
    end_time = datetime.now()
    duration = end_time - start_time
    
    print("\n" + "=" * 60)
    print("ğŸ“‹ å¿«é€Ÿå¯åŠ¨æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    success_count = 0
    for test_name, success in results:
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
        print(f"   {test_name}: {status}")
        if success:
            success_count += 1
    
    success_rate = success_count / len(results) * 100
    
    print(f"\nğŸ¯ æˆåŠŸç‡: {success_count}/{len(results)} ({success_rate:.0f}%)")
    print(f"â±ï¸  æ€»è€—æ—¶: {duration}")
    
    if success_rate >= 80:
        print("\nğŸ‰ é¡¹ç›®åŠŸèƒ½éªŒè¯æˆåŠŸï¼æ‚¨å¯ä»¥å¼€å§‹ä½¿ç”¨å®Œæ•´åŠŸèƒ½äº†ã€‚")
        print("\nğŸ“š ä¸‹ä¸€æ­¥å»ºè®®:")
        print("   1. è¿è¡Œå®Œæ•´åˆ†æ: python scripts/main_analysis_pipeline.py")
        print("   2. æŸ¥çœ‹æ¼”ç¤ºè„šæœ¬: python scripts/han_demo.py --use-sample")
        print("   3. å‡†å¤‡çœŸå®æ•°æ®å¹¶è¿è¡Œå®Œæ•´æµç¨‹")
        
    elif success_rate >= 50:
        print("\nâš ï¸  éƒ¨åˆ†åŠŸèƒ½æ­£å¸¸ï¼Œå»ºè®®æ£€æŸ¥å¤±è´¥çš„ç»„ä»¶")
        print("   å¯èƒ½éœ€è¦å®‰è£…é¢å¤–çš„ä¾èµ–åŒ…")
        
    else:
        print("\nâŒ å¤šä¸ªç»„ä»¶å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®")
        print("   å»ºè®®å…ˆè¿è¡Œ: python scripts/check_environment.py")
    
    print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶ä½ç½®:")
    print("   - ç¤ºä¾‹æ•°æ®: data/processed/final_analysis_data/")
    print("   - åˆ†æç»“æœ: results/analysis_output/")
    print("   - å¿«é€ŸæŠ¥å‘Š: results/analysis_output/quick_start_report.json")
    
    return success_rate >= 50

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
