#!/usr/bin/env python
"""
å¼‚æ„æ³¨æ„åŠ›ç½‘ç»œ(HAN)æ¼”ç¤ºè„šæœ¬
å±•ç¤ºå¦‚ä½•ä½¿ç”¨HANæ¨¡å‹è¿›è¡Œå¼€æºç”Ÿæ€ç³»ç»Ÿåˆ†æ
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

import pandas as pd
import numpy as np
import networkx as nx
import torch
from datetime import datetime

# å¯¼å…¥HANæ¨¡å—
from src.advanced_models.han_model import HANAnalyzer, HeteroGraphBuilder, HANModel, HANTrainer
from src.advanced_models.graph_embedding import ComparisonAnalyzer
from src.utils.logging_config import setup_logger
from config.settings import NETWORK_OUTPUT_DIR, FILENAMES, ANALYSIS_OUTPUT_DIR

# è®¾ç½®æ—¥å¿—
logger = setup_logger(__name__)


def create_sample_heterogeneous_graph() -> nx.Graph:
    """åˆ›å»ºç¤ºä¾‹å¼‚æ„å›¾"""
    print("ğŸ”§ åˆ›å»ºç¤ºä¾‹å¼‚æ„å›¾...")
    
    G = nx.Graph()
    
    # æ·»åŠ ä¸åŒç±»å‹çš„èŠ‚ç‚¹
    # ç”¨æˆ·èŠ‚ç‚¹
    users = [f"user_{i}" for i in range(50)]
    for user in users:
        G.add_node(user, type='user', user_id=user)
    
    # ä»“åº“èŠ‚ç‚¹
    repos = [f"repo_{i}" for i in range(20)]
    for repo in repos:
        G.add_node(repo, type='repo', 
                  stars=np.random.randint(0, 1000),
                  forks=np.random.randint(0, 100),
                  primary_role=np.random.choice(['application', 'library', 'tool']))
    
    # PRèŠ‚ç‚¹
    prs = [f"pr_{i}" for i in range(30)]
    for pr in prs:
        G.add_node(pr, type='pr', timestamp='2023-01-01')
    
    # IssueèŠ‚ç‚¹
    issues = [f"issue_{i}" for i in range(40)]
    for issue in issues:
        G.add_node(issue, type='issue', timestamp='2023-01-01')
    
    # æ·»åŠ è¾¹
    # ç”¨æˆ·-ä»“åº“ï¼ˆStarå…³ç³»ï¼‰
    for user in users[:30]:  # å‰30ä¸ªç”¨æˆ·
        for repo in np.random.choice(repos, size=np.random.randint(1, 5), replace=False):
            G.add_edge(user, repo, type='star')
    
    # ç”¨æˆ·-PRï¼ˆåˆ›å»ºå…³ç³»ï¼‰
    for pr in prs:
        user = np.random.choice(users)
        repo = np.random.choice(repos)
        G.add_edge(user, pr, type='create')
        G.add_edge(pr, repo, type='belongs_to')
    
    # ç”¨æˆ·-Issueï¼ˆåˆ›å»ºå…³ç³»ï¼‰
    for issue in issues:
        user = np.random.choice(users)
        repo = np.random.choice(repos)
        G.add_edge(user, issue, type='create')
        G.add_edge(issue, repo, type='belongs_to')
    
    print(f"âœ… ç¤ºä¾‹å›¾åˆ›å»ºå®Œæˆ: {G.number_of_nodes()} ä¸ªèŠ‚ç‚¹, {G.number_of_edges()} æ¡è¾¹")
    
    # æ‰“å°èŠ‚ç‚¹ç±»å‹ç»Ÿè®¡
    node_types = {}
    for node, data in G.nodes(data=True):
        node_type = data.get('type', 'unknown')
        node_types[node_type] = node_types.get(node_type, 0) + 1
    
    print("ğŸ“Š èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ:")
    for node_type, count in node_types.items():
        print(f"   - {node_type}: {count} ä¸ª")
    
    return G


def demonstrate_hetero_graph_building(graph: nx.Graph):
    """æ¼”ç¤ºå¼‚æ„å›¾æ„å»º"""
    print("\n" + "=" * 60)
    print("1. å¼‚æ„å›¾æ„å»ºæ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºå¼‚æ„å›¾æ„å»ºå™¨
    builder = HeteroGraphBuilder(graph)
    
    # æ„å»ºå¼‚æ„æ•°æ®
    hetero_data = builder.build_hetero_data()
    
    print(f"âœ… å¼‚æ„å›¾æ„å»ºå®Œæˆ!")
    print(f"   - èŠ‚ç‚¹ç±»å‹æ•°: {len(hetero_data.node_types)}")
    print(f"   - è¾¹ç±»å‹æ•°: {len(hetero_data.edge_types)}")
    
    # è¯¦ç»†ä¿¡æ¯
    print(f"\nğŸ“‹ è¯¦ç»†ä¿¡æ¯:")
    for node_type in hetero_data.node_types:
        num_nodes = hetero_data[node_type].num_nodes
        feature_dim = hetero_data[node_type].x.shape[1] if hasattr(hetero_data[node_type], 'x') else 0
        print(f"   - {node_type}: {num_nodes} ä¸ªèŠ‚ç‚¹, {feature_dim} ç»´ç‰¹å¾")
    
    for edge_type in hetero_data.edge_types:
        num_edges = hetero_data[edge_type].edge_index.shape[1]
        print(f"   - {edge_type}: {num_edges} æ¡è¾¹")
    
    return hetero_data


def demonstrate_han_model_creation(hetero_data):
    """æ¼”ç¤ºHANæ¨¡å‹åˆ›å»º"""
    print("\n" + "=" * 60)
    print("2. HANæ¨¡å‹åˆ›å»ºæ¼”ç¤º")
    print("=" * 60)
    
    # è·å–å…ƒæ•°æ®
    metadata = (hetero_data.node_types, hetero_data.edge_types)
    
    # åˆ›å»ºHANæ¨¡å‹
    model = HANModel(
        metadata=metadata,
        hidden_dim=64,  # ä½¿ç”¨è¾ƒå°çš„ç»´åº¦ä»¥èŠ‚çœè®¡ç®—èµ„æº
        num_heads=4,
        num_layers=2,
        dropout=0.1,
        num_classes=3  # ä»“åº“åˆ†ç±»ä»»åŠ¡
    )
    
    print(f"âœ… HANæ¨¡å‹åˆ›å»ºå®Œæˆ!")
    print(f"   - éšè—ç»´åº¦: {model.hidden_dim}")
    print(f"   - æ³¨æ„åŠ›å¤´æ•°: {model.num_heads}")
    print(f"   - å±‚æ•°: {model.num_layers}")
    print(f"   - åˆ†ç±»ç±»åˆ«æ•°: {model.num_classes}")
    
    # æ‰“å°æ¨¡å‹ç»“æ„
    print(f"\nğŸ—ï¸ æ¨¡å‹ç»“æ„:")
    print(model)
    
    # ç»Ÿè®¡å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"\nğŸ“Š æ¨¡å‹å‚æ•°:")
    print(f"   - æ€»å‚æ•°æ•°: {total_params:,}")
    print(f"   - å¯è®­ç»ƒå‚æ•°æ•°: {trainable_params:,}")
    
    return model


def demonstrate_han_training(model, hetero_data):
    """æ¼”ç¤ºHANæ¨¡å‹è®­ç»ƒ"""
    print("\n" + "=" * 60)
    print("3. HANæ¨¡å‹è®­ç»ƒæ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = HANTrainer(
        model=model,
        hetero_data=hetero_data,
        task_type='node_classification',
        target_node_type='repo',  # å¯¹ä»“åº“èŠ‚ç‚¹è¿›è¡Œåˆ†ç±»
        learning_rate=0.01
    )
    
    print(f"ğŸ¯ è®­ç»ƒè®¾ç½®:")
    print(f"   - ä»»åŠ¡ç±»å‹: {trainer.task_type}")
    print(f"   - ç›®æ ‡èŠ‚ç‚¹ç±»å‹: {trainer.target_node_type}")
    print(f"   - å­¦ä¹ ç‡: {trainer.learning_rate}")
    print(f"   - è®¾å¤‡: {trainer.device}")
    
    # å‡†å¤‡èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡
    print(f"\nğŸ“‹ å‡†å¤‡èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡...")
    task_data = trainer.prepare_node_classification_task()
    
    train_count = task_data['train_mask'].sum().item()
    val_count = task_data['val_mask'].sum().item()
    test_count = task_data['test_mask'].sum().item()
    
    print(f"   - è®­ç»ƒé›†: {train_count} ä¸ªèŠ‚ç‚¹")
    print(f"   - éªŒè¯é›†: {val_count} ä¸ªèŠ‚ç‚¹")
    print(f"   - æµ‹è¯•é›†: {test_count} ä¸ªèŠ‚ç‚¹")
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    training_history = trainer.train(epochs=50, patience=10, verbose=True)
    
    print(f"âœ… è®­ç»ƒå®Œæˆ!")
    print(f"   - è®­ç»ƒè½®æ•°: {len(training_history['train_loss'])}")
    print(f"   - æœ€ç»ˆè®­ç»ƒæŸå¤±: {training_history['train_loss'][-1]:.4f}")
    print(f"   - æœ€ç»ˆéªŒè¯æŸå¤±: {training_history['val_loss'][-1]:.4f}")
    print(f"   - æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {training_history['train_acc'][-1]:.4f}")
    print(f"   - æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {training_history['val_acc'][-1]:.4f}")
    
    return trainer, training_history


def demonstrate_model_evaluation(trainer):
    """æ¼”ç¤ºæ¨¡å‹è¯„ä¼°"""
    print("\n" + "=" * 60)
    print("4. æ¨¡å‹è¯„ä¼°æ¼”ç¤º")
    print("=" * 60)
    
    # æµ‹è¯•æ¨¡å‹
    print("ğŸ§ª æµ‹è¯•æ¨¡å‹æ€§èƒ½...")
    test_results = trainer.test()
    
    print(f"âœ… æµ‹è¯•å®Œæˆ!")
    print(f"   - æµ‹è¯•æŸå¤±: {test_results['test_loss']:.4f}")
    print(f"   - æµ‹è¯•å‡†ç¡®ç‡: {test_results['test_accuracy']:.4f}")
    
    # æå–æ³¨æ„åŠ›æƒé‡
    print(f"\nğŸ” æå–æ³¨æ„åŠ›æƒé‡...")
    attention_weights = trainer.extract_attention_weights()
    
    print(f"   - æ•è·çš„æ³¨æ„åŠ›å±‚æ•°: {len(attention_weights)}")
    
    for layer_name, weights in attention_weights.items():
        if isinstance(weights, dict):
            print(f"   - {layer_name}: {len(weights)} ä¸ªè¾¹ç±»å‹")
            for edge_type, edge_attention in weights.items():
                if hasattr(edge_attention, 'shape'):
                    print(f"     * {edge_type}: {edge_attention.shape}")
    
    return test_results, attention_weights


def demonstrate_attention_analysis(attention_weights):
    """æ¼”ç¤ºæ³¨æ„åŠ›æƒé‡åˆ†æ"""
    print("\n" + "=" * 60)
    print("5. æ³¨æ„åŠ›æƒé‡åˆ†ææ¼”ç¤º")
    print("=" * 60)
    
    if not attention_weights:
        print("âš ï¸  æ²¡æœ‰æ³¨æ„åŠ›æƒé‡æ•°æ®å¯åˆ†æ")
        return {}
    
    analysis_results = {}
    
    print("ğŸ“Š åˆ†ææ³¨æ„åŠ›æƒé‡ç»Ÿè®¡...")
    
    for layer_name, layer_weights in attention_weights.items():
        print(f"\nğŸ” {layer_name}:")
        layer_analysis = {}
        
        if isinstance(layer_weights, dict):
            for edge_type, edge_weights in layer_weights.items():
                if isinstance(edge_weights, torch.Tensor):
                    # ç§»åˆ°CPUå¹¶è½¬æ¢ä¸ºnumpy
                    weights_np = edge_weights.detach().cpu().numpy()
                    
                    stats = {
                        'mean': float(np.mean(weights_np)),
                        'std': float(np.std(weights_np)),
                        'min': float(np.min(weights_np)),
                        'max': float(np.max(weights_np)),
                        'shape': weights_np.shape
                    }
                    
                    layer_analysis[str(edge_type)] = stats
                    
                    print(f"   - {edge_type}:")
                    print(f"     * å½¢çŠ¶: {stats['shape']}")
                    print(f"     * å‡å€¼: {stats['mean']:.4f}")
                    print(f"     * æ ‡å‡†å·®: {stats['std']:.4f}")
                    print(f"     * èŒƒå›´: [{stats['min']:.4f}, {stats['max']:.4f}]")
        
        analysis_results[layer_name] = layer_analysis
    
    print(f"\nâœ… æ³¨æ„åŠ›æƒé‡åˆ†æå®Œæˆ!")
    return analysis_results


def demonstrate_graph_embedding_comparison(graph):
    """æ¼”ç¤ºå›¾åµŒå…¥æ–¹æ³•æ¯”è¾ƒ"""
    print("\n" + "=" * 60)
    print("6. å›¾åµŒå…¥æ–¹æ³•æ¯”è¾ƒæ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºæ¯”è¾ƒåˆ†æå™¨
    comparator = ComparisonAnalyzer(graph)
    
    print("ğŸ”„ å¼€å§‹æ¯”è¾ƒä¸åŒçš„å›¾åµŒå…¥æ–¹æ³•...")
    print("   (ä½¿ç”¨è¾ƒå°çš„å‚æ•°ä»¥èŠ‚çœæ—¶é—´)")
    
    # è¿è¡Œæ¯”è¾ƒåˆ†æ
    comparison_results = comparator.compare_embedding_methods(embedding_dim=32)
    
    print(f"\nâœ… å›¾åµŒå…¥æ¯”è¾ƒå®Œæˆ!")
    
    # æ‰“å°æ¯”è¾ƒç»“æœ
    for method_name, results in comparison_results.items():
        if 'quality_metrics' in results:
            metrics = results['quality_metrics']
            print(f"\nğŸ“Š {method_name}:")
            print(f"   - åµŒå…¥ç»´åº¦: {metrics['embedding_dimension']}")
            print(f"   - èŠ‚ç‚¹æ•°: {metrics['num_nodes']}")
            print(f"   - å¹³å‡èŒƒæ•°: {metrics['mean_norm']:.4f}")
            print(f"   - å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦: {metrics['mean_cosine_similarity']:.4f}")
            print(f"   - ç¨€ç–åº¦: {metrics['sparsity_ratio']:.4f}")
    
    return comparison_results


def demonstrate_complete_han_pipeline(use_sample_graph: bool = True):
    """æ¼”ç¤ºå®Œæ•´çš„HANåˆ†ææµæ°´çº¿"""
    print("ğŸš€ å¼‚æ„æ³¨æ„åŠ›ç½‘ç»œ(HAN)å®Œæ•´æ¼”ç¤º")
    print("=" * 80)
    
    start_time = datetime.now()
    
    try:
        # 1. å‡†å¤‡å›¾æ•°æ®
        if use_sample_graph:
            graph = create_sample_heterogeneous_graph()
        else:
            # å°è¯•åŠ è½½çœŸå®å›¾æ•°æ®
            graph_path = NETWORK_OUTPUT_DIR / FILENAMES["graph_file"]
            if graph_path.exists():
                graph = nx.read_graphml(str(graph_path))
                print(f"âœ… åŠ è½½çœŸå®å›¾æ•°æ®: {graph.number_of_nodes()} ä¸ªèŠ‚ç‚¹")
                
                # å¦‚æœå›¾å¤ªå¤§ï¼Œé‡‡æ ·å­å›¾
                if graph.number_of_nodes() > 200:
                    print("ğŸ“ å›¾è¾ƒå¤§ï¼Œé‡‡æ ·å­å›¾è¿›è¡Œæ¼”ç¤º...")
                    nodes_sample = list(graph.nodes())[:200]
                    graph = graph.subgraph(nodes_sample).copy()
                    print(f"   é‡‡æ ·å: {graph.number_of_nodes()} ä¸ªèŠ‚ç‚¹")
            else:
                print("âš ï¸  æœªæ‰¾åˆ°çœŸå®å›¾æ•°æ®ï¼Œä½¿ç”¨ç¤ºä¾‹å›¾")
                graph = create_sample_heterogeneous_graph()
        
        # 2. å¼‚æ„å›¾æ„å»º
        hetero_data = demonstrate_hetero_graph_building(graph)
        
        # 3. HANæ¨¡å‹åˆ›å»º
        model = demonstrate_han_model_creation(hetero_data)
        
        # 4. æ¨¡å‹è®­ç»ƒ
        trainer, training_history = demonstrate_han_training(model, hetero_data)
        
        # 5. æ¨¡å‹è¯„ä¼°
        test_results, attention_weights = demonstrate_model_evaluation(trainer)
        
        # 6. æ³¨æ„åŠ›æƒé‡åˆ†æ
        attention_analysis = demonstrate_attention_analysis(attention_weights)
        
        # 7. å›¾åµŒå…¥æ–¹æ³•æ¯”è¾ƒ
        embedding_comparison = demonstrate_graph_embedding_comparison(graph)
        
        # 8. ä¿å­˜ç»“æœ
        print("\n" + "=" * 60)
        print("7. ä¿å­˜åˆ†æç»“æœ")
        print("=" * 60)
        
        # ä¿å­˜ç»¼åˆç»“æœ
        results_summary = {
            'graph_info': {
                'num_nodes': graph.number_of_nodes(),
                'num_edges': graph.number_of_edges(),
                'node_types': len(hetero_data.node_types),
                'edge_types': len(hetero_data.edge_types)
            },
            'model_info': {
                'hidden_dim': model.hidden_dim,
                'num_heads': model.num_heads,
                'num_layers': model.num_layers,
                'total_parameters': sum(p.numel() for p in model.parameters())
            },
            'training_results': {
                'epochs_trained': len(training_history['train_loss']),
                'final_train_loss': training_history['train_loss'][-1],
                'final_val_loss': training_history['val_loss'][-1],
                'final_train_acc': training_history['train_acc'][-1],
                'final_val_acc': training_history['val_acc'][-1]
            },
            'test_results': test_results,
            'attention_analysis': attention_analysis,
            'embedding_comparison': {
                method: {'quality_metrics': results.get('quality_metrics', {})}
                for method, results in embedding_comparison.items()
            }
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        import json
        results_path = ANALYSIS_OUTPUT_DIR / "han_demo_results.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results_summary, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ“ åˆ†æç»“æœå·²ä¿å­˜è‡³: {results_path}")
        
        # 9. æ€»ç»“
        end_time = datetime.now()
        duration = end_time - start_time
        
        print("\n" + "=" * 80)
        print("âœ… HANæ¼”ç¤ºå®Œæˆ!")
        print(f"   - æ€»è€—æ—¶: {duration}")
        print(f"   - å›¾è§„æ¨¡: {graph.number_of_nodes()} ä¸ªèŠ‚ç‚¹, {graph.number_of_edges()} æ¡è¾¹")
        print(f"   - å¼‚æ„å›¾: {len(hetero_data.node_types)} ç§èŠ‚ç‚¹ç±»å‹, {len(hetero_data.edge_types)} ç§è¾¹ç±»å‹")
        print(f"   - æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
        print(f"   - æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_results['test_accuracy']:.4f}")
        print(f"   - ç»“æœæ–‡ä»¶: {results_path}")
        print("=" * 80)
        
        return results_summary
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        logger.error(f"HANæ¼”ç¤ºå¤±è´¥: {e}")
        return None


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='HANæ¨¡å‹æ¼”ç¤ºè„šæœ¬')
    parser.add_argument('--use-sample', action='store_true', 
                       help='ä½¿ç”¨ç¤ºä¾‹å›¾æ•°æ®ï¼ˆé»˜è®¤å°è¯•åŠ è½½çœŸå®æ•°æ®ï¼‰')
    parser.add_argument('--sample-only', action='store_true',
                       help='ä»…ä½¿ç”¨ç¤ºä¾‹å›¾æ•°æ®')
    
    args = parser.parse_args()
    
    # è¿è¡Œæ¼”ç¤º
    results = demonstrate_complete_han_pipeline(use_sample_graph=args.sample_only)
    
    if results:
        print("\nğŸ‰ æ¼”ç¤ºæˆåŠŸå®Œæˆï¼")
        print("ğŸ“Š ä¸»è¦ç»“æœ:")
        print(f"   - æµ‹è¯•å‡†ç¡®ç‡: {results['test_results']['test_accuracy']:.4f}")
        print(f"   - è®­ç»ƒè½®æ•°: {results['training_results']['epochs_trained']}")
        print(f"   - æ¨¡å‹å‚æ•°é‡: {results['model_info']['total_parameters']:,}")
    else:
        print("\nâŒ æ¼”ç¤ºå¤±è´¥")


if __name__ == "__main__":
    main()
